{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "BTCNN_PATH = os.path.join(ROOT_PATH, \"btcnn\")\n",
    "HBO_BENCH_PATH = os.path.join(ROOT_PATH, \"hbo_bench\")\n",
    "\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "EXPERIMENT_PATH = os.getcwd()\n",
    "ARTIFACTS_PATH = os.path.join(EXPERIMENT_PATH, \"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from json import load, dumps, dump\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from utils import get_logical_tree, get_full_plan, get_selectivities, extract_list_info\n",
    "from oracle import Oracle, OracleRequest, TIMEOUT\n",
    "from data_config import HINTSETS, DOPS, HINTS, DEFAULT_HINTSET\n",
    "from data_types import ExplainNode\n",
    "from regressor import BinaryTreeRegressor\n",
    "from vectorization import extract_vertices_and_edges, ALL_FEATURES\n",
    "from dataset import WeightedBinaryTreeDataset, weighted_binary_tree_collate\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = len(ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: test that model can be loaded; checklosses; rename models and experiments; iterate over architectures; iterate over data splits, check assert for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Are convolution layers useful?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that convolutional layers indeed help to better represent queries, simplifying the further modelling task. To do this, we will compare the results of training neural networks with different complexity ratios between the convolution and regression parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_name(name):\n",
    "    name = name.replace('_', ', ')\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])'\n",
    "    return re.sub(pattern, ' ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_name(\"SmallBTCNN_MediumFCNN_InstanceNormalisation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groupped_curves(clearml_data):\n",
    "    group_to_list_x = defaultdict(list)\n",
    "    group_to_list_y = defaultdict(list)\n",
    "    for curve in clearml_data:\n",
    "        group_to_list_x[curve['name']].append(curve['x'])\n",
    "        group_to_list_y[curve['name']].append(curve['y'])\n",
    "    return group_to_list_x, group_to_list_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_curves(\n",
    "    clearml_data, \n",
    "    logscale=False, \n",
    "    skip_first_epocs=True, \n",
    "    start_point=0,\n",
    "    end_point=60,\n",
    "    figsize=(12, 6),\n",
    "    label_fontsize=15, \n",
    "    linewidth=4,\n",
    "    scattersize=2,\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    title_fontsize=18, \n",
    "    tick_label_fontsize=15, \n",
    "    legend_fontsize=12, \n",
    "    save_name=None,\n",
    "):\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_palette(\"deep\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    group_to_list_x, group_to_list_y = get_groupped_curves(clearml_data)\n",
    "    for i, group_name in enumerate(group_to_list_x.keys()):\n",
    "        list_x, list_y = group_to_list_x[group_name], group_to_list_y[group_name]\n",
    "        x = list_x[0]\n",
    "\n",
    "        is_it_train_data = len(list_x[0]) == 300 + 1 or len(list_x[0]) == 150 + 1\n",
    "        if is_it_train_data:\n",
    "            x = x[::5]\n",
    "            list_y = [el[::5] for el in list_y]\n",
    "        \n",
    "        x = x[start_point:end_point]\n",
    "        list_y = [el[start_point:end_point] for el in list_y]\n",
    "            \n",
    "        if logscale:\n",
    "            mean_y = np.mean(np.log(np.array(list_y)), axis=0)\n",
    "        else:\n",
    "            mean_y = np.mean(np.array(list_y), axis=0)\n",
    "        \n",
    "        color = sns.color_palette(\"deep\")[i % len(sns.color_palette(\"deep\"))]\n",
    "\n",
    "        ax.scatter(x, mean_y, alpha=1.0, linewidth=scattersize, label=group_name, color=color)\n",
    "        ax.plot(x, mean_y, alpha=0.5, linewidth=linewidth, color=color)\n",
    "        \n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.2)\n",
    "    ax.set_xlabel(xlabel, fontsize=label_fontsize)\n",
    "    ax.set_ylabel(ylabel + (\" (logscale)\" if logscale else \"\"), fontsize=label_fontsize)\n",
    "    ax.legend(loc=\"best\", fontsize=legend_fontsize)\n",
    "    ax.tick_params(axis='x', labelsize=tick_label_fontsize)\n",
    "    ax.tick_params(axis='y', labelsize=tick_label_fontsize)\n",
    "\n",
    "    if save_name:\n",
    "        plt.savefig(f\"{ARTIFACTS_PATH}/{save_name}\", format='svg', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sizes_from_name(name):\n",
    "    name_wo_run = re.search(r'(.*?)_(\\d+)$', name).group(1)\n",
    "    name_wo_run = name_wo_run.replace('_', ', ')\n",
    "    pattern = r'([a-z])([A-Z])'\n",
    "    formatted_name = re.sub(pattern, r'\\1 \\2', name_wo_run)\n",
    "    components = formatted_name.split(', ')\n",
    "    formatted_components = ', '.join(components[:2])\n",
    "    return formatted_components    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_type in [\"train\", \"val\", \"test\", \"ood\"]:\n",
    "    with open(f\"{ARTIFACTS_PATH}/losses/medium_models/{loss_type}.json\", \"r\") as f:\n",
    "        clearml_data = load(f)\n",
    "        for l in clearml_data:\n",
    "            l['name'] = extract_sizes_from_name(l['name'])\n",
    "    plot_curves(clearml_data, xlabel=\"epoch\", ylabel=\"MSE\", logscale=False, start_point=0, save_name=f\"medium_models_{loss_type}.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of having binary tree convolutional layers are obvious. So it makes sense to select the architecture further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Which model architecture is the best?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_norm_layer_from_name(name):\n",
    "    norm_layer = name.split('_')[-2]\n",
    "    if norm_layer == \"BigFCNN\":\n",
    "        norm_layer = \"No Normalisation\" \n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_type in [\"train\", \"val\", \"test\", \"ood\"]:\n",
    "    with open(f\"{ARTIFACTS_PATH}/losses/big_models/{loss_type}.json\", \"r\") as f:\n",
    "        clearml_data = load(f)\n",
    "        for l in clearml_data:\n",
    "            l['name'] = extract_norm_layer_from_name(l['name'])\n",
    "    plot_curves(clearml_data, xlabel=\"epoch\", ylabel=\"MSE\", logscale=True, start_point=0, save_name=f\"big_models_{loss_type}.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`InstanceNorm` slightly outperforms other normalistaion layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Performance on noised data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noise_type(name):\n",
    "    match = re.search(r\"\\[(.*?)\\]\", name)\n",
    "    if not match:\n",
    "        return \"No Noise\"\n",
    "    result = match.group(1)\n",
    "    if result == \"all\":\n",
    "        return \"Noised Operations, Statistics\"\n",
    "    elif result == \"ops\":\n",
    "        return \"Noised Operations\"\n",
    "    else:\n",
    "        return \"Noised Statistics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_type in [\"train\", \"val\", \"test\", \"ood\"]:\n",
    "    with open(f\"{ARTIFACTS_PATH}/losses/noised_data/{loss_type}.json\", \"r\") as f:\n",
    "        clearml_data = load(f)\n",
    "        for l in clearml_data:\n",
    "            l['name'] = extract_noise_type(l['name'])\n",
    "    plot_curves(clearml_data, xlabel=\"epoch\", ylabel=\"MSE\", logscale=False, start_point=0, save_name=f\"big_models_{loss_type}_on_noised_data.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected - there is no generalisation when training on full noise, the network only memorises examples.\n",
    "When the statistical channels are noisy, it becomes much more difficult for the NN to predict the execution time. Not surprisingly, because these statistics are estimates of the size of the data being processed, derived from the cardinality model. They take into account the statistics in the database and greatly simplify the modelling of runtime dependence. In a sense, the NN only trains the correct cost GUQs.\n",
    "Note, that if both operations and statistics are available, then NN model can learn contextual dependency between plan and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion.** The working hypothesis is that the trained NN actually plays the role of a complex (with contextual by operations dependency) **cost corrector model**, where a time scale is used instead of an abstract cost scale. In this case, the question arises:\n",
    "\n",
    "*\"Is the quality of the trained model on validation / test set better than the quality of cost model?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_type in [\"train\", \"val\", \"test\", \"ood\"]:\n",
    "    with open(f\"{ARTIFACTS_PATH}/losses/noised_data/{loss_type}.json\", \"r\") as f:\n",
    "        clearml_data = load(f)\n",
    "    with open(f\"{ARTIFACTS_PATH}/losses/big_models/{loss_type}.json\", \"r\") as f:\n",
    "        clearml_data += load(f)[-10:-5]\n",
    "    for l in clearml_data:\n",
    "        l['name'] = extract_noise_type(l['name'])\n",
    "    plot_curves(clearml_data, xlabel=\"epoch\", ylabel=\"MSE\", end_point=30, logscale=True, start_point=0, save_name=f\"complex_landscape.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that replacing the tree information with full noise even makes the optimisation problem easier - this is a clear signal that the natural landscape of the loss function, which is given by real statistics and plans, is extremely difficult to be approximated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
