{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning!**\n",
    "\n",
    "Hi reader! This is a copy of a notebook from the `explore/12-tcnn-abilities` branch (`training.ipynb`), in which I only changed some procedure calls (as they are now in separate packages as `hbo_bench`, `btcnn`). I did not repeat the training itself, and I took the model weights and error values during the training from `clearml` logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "BTCNN_PATH = os.path.join(ROOT_PATH, \"btcnn\")\n",
    "HBO_BENCH_PATH = os.path.join(ROOT_PATH, \"hbo_bench\")\n",
    "\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "sys.path.insert(0, BTCNN_PATH)\n",
    "sys.path.insert(0, HBO_BENCH_PATH)\n",
    "\n",
    "EXPERIMENT_PATH = os.getcwd()\n",
    "ARTIFACTS_PATH = os.path.join(EXPERIMENT_PATH, \"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from json import load, dumps, dump\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from utils import get_logical_tree, get_full_plan, get_selectivities, extract_list_info\n",
    "from oracle import Oracle, OracleRequest, TIMEOUT\n",
    "from data_config import HINTSETS, DOPS, HINTS, DEFAULT_HINTSET\n",
    "from data_types import ExplainNode\n",
    "from regressor import BinaryTreeRegressor\n",
    "from vectorization import extract_vertices_and_edges, ALL_FEATURES\n",
    "from dataset import WeightedBinaryTreeDataset, weighted_binary_tree_collate\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Preprocessing plans\n",
    "Here we load from Oracle (for `JOB` and `SQ`) data with queries and their plans under different parameters and then processing these plans to pairs vertices, edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_oracle = Oracle(f\"{HBO_BENCH_PATH}/data/processed/JOB\")\n",
    "\n",
    "job_list_info = extract_list_info(oracle=job_oracle, query_names=job_oracle.get_query_names())\n",
    "job_list_vertices, job_list_edges, job_list_time = [[info[key] for info in job_list_info] for key in [\"vertices\", \"edges\", \"time\"]]\n",
    "\n",
    "job_max_possible_size = len(job_oracle.get_query_names()) * len(DOPS) * len(HINTSETS)\n",
    "print(f\"[JOB]: dataset size is {len(job_list_info)} / {job_max_possible_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_oracle = Oracle(f\"{HBO_BENCH_PATH}/data/processed/sample_queries\")\n",
    "\n",
    "sq_list_info = extract_list_info(oracle=sq_oracle, query_names=sq_oracle.get_query_names())\n",
    "sq_list_vertices, sq_list_edges, sq_list_time = [[info[key] for info in sq_list_info] for key in [\"vertices\", \"edges\", \"time\"]]\n",
    "\n",
    "sq_max_possible_size = len(sq_oracle.get_query_names()) * len(DOPS) * len(HINTSETS)\n",
    "print(f\"[SQ]: dataset size is {len(sq_list_info)} / {sq_max_possible_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_X = set([(str(v.flatten().tolist()), str(e.flatten().tolist())) for v, e in zip(job_list_vertices, job_list_edges)])\n",
    "sq_X = set([(str(v.flatten().tolist()), str(e.flatten().tolist())) for v, e in zip(sq_list_vertices, sq_list_edges)])\n",
    "print(f\"Around {100 * len(sq_X & job_X) / len(sq_X):0.1f} % of plans from SQ bench exists in JOB bench\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Splitting data to train-val / test sets\n",
    "\n",
    "Since the queries from `SQ` are also over imdb data, we can use them as a test dataset. However, some of them are very different in nature from queries from `JOB`, so we will create two versions of the test dataset - in-distrubtion and out-of-distribution versions. We will consider a query as a in-distribution (`id` query) if its default logical tree occurs among the default logical trees of `JOB`s queries, otherwise we will call it a out-of-distribution (`ood`) query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_logical_trees = set(\n",
    "    get_logical_tree(job_oracle.get_explain_plan(OracleRequest(query_name=query_name, hintset=hintset, dop=dop)))\n",
    "    for query_name in job_oracle.get_query_names() for hintset in HINTSETS for dop in DOPS\n",
    ")\n",
    "\n",
    "ood_sq_list_info, id_sq_list_info = [], []\n",
    "for info in sq_list_info:\n",
    "    logical_tree = get_logical_tree(sq_oracle.get_explain_plan(OracleRequest(query_name=info[\"query_name\"], hintset=info[\"hintset\"], dop=info[\"dop\"])))\n",
    "    if logical_tree not in job_logical_trees:\n",
    "        ood_sq_list_info.append(info)\n",
    "    else:\n",
    "        id_sq_list_info.append(info)\n",
    "        \n",
    "ood_sq_list_vertices, ood_sq_list_edges, ood_sq_list_time = [[info[key] for info in ood_sq_list_info] for key in [\"vertices\", \"edges\", \"time\"]]\n",
    "id_sq_list_vertices, id_sq_list_edges, id_sq_list_time = [[info[key] for info in id_sq_list_info] for key in [\"vertices\", \"edges\", \"time\"]]\n",
    "assert len(ood_sq_list_info) + len(id_sq_list_info) == len(sq_list_info), \"Something went wrong ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(job_list_info, f\"{ARTIFACTS_PATH}/job_list_info\")\n",
    "torch.save(ood_sq_list_info, f\"{ARTIFACTS_PATH}/ood_sq_list_info\")\n",
    "torch.save(id_sq_list_info, f\"{ARTIFACTS_PATH}/id_sq_list_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Creating `torch.Datasets`\n",
    "We see that most of the plans are repetitive. This is the reason for switching to weighted datasets (and dataloaders) to speed up the learning process. It also simplifies the landscape, leaving the solving of collision problem to the preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_all_plans = [\n",
    "    get_full_plan(job_oracle.get_explain_plan(OracleRequest(query_name=query_name, hintset=hintset, dop=dop)))\n",
    "    for query_name in job_oracle.get_query_names() for hintset in HINTSETS for dop in DOPS\n",
    "]\n",
    "print(f\"[JOB]: total # of unique plans: {len(set(job_all_plans))} / {len(job_all_plans)}\")\n",
    "\n",
    "sq_all_plans = [\n",
    "    get_full_plan(sq_oracle.get_explain_plan(OracleRequest(query_name=query_name, hintset=hintset, dop=dop)))\n",
    "    for query_name in sq_oracle.get_query_names() for hintset in HINTSETS for dop in DOPS\n",
    "]\n",
    "print(f\"[SQ]: total # of unique plans: {len(set(sq_all_plans))} / {len(sq_all_plans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_weighted_dataset = WeightedBinaryTreeDataset(job_list_vertices, job_list_edges, job_list_time, DEVICE)\n",
    "print(f\"[JOB]: total # of unique plans in the weighted dataset {len(job_weighted_dataset)}\")\n",
    "\n",
    "ood_sq_weighted_dataset = WeightedBinaryTreeDataset(ood_sq_list_vertices, ood_sq_list_edges, ood_sq_list_time, DEVICE)\n",
    "print(f\"[SQ, OOD]: total # of unique plans in the weighted dataset {len(ood_sq_weighted_dataset)}\")\n",
    "\n",
    "id_sq_weighted_dataset = WeightedBinaryTreeDataset(id_sq_list_vertices, id_sq_list_edges, id_sq_list_time, DEVICE)\n",
    "print(f\"[SQ, ID]: total # of unique plans in the weighted dataset {len(id_sq_weighted_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up some grid of architectures to be searched.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = len(ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_btcnn = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "small_btcnn = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, in_channels),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "medium_btcnn = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, 128),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(128, in_channels),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "small_fcnn = lambda: torch.nn.Sequential(\n",
    "    nn.Linear(in_channels, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ")\n",
    "\n",
    "medium_fcnn = lambda: torch.nn.Sequential(\n",
    "    nn.Linear(in_channels, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    return [\n",
    "        BinaryTreeRegressor(no_btcnn(), small_fcnn(), \"NoBTCNN_SmallFCNN\"),\n",
    "        BinaryTreeRegressor(no_btcnn(), medium_fcnn(), \"NoBTCNN_MediumFCNN\"),\n",
    "        BinaryTreeRegressor(small_btcnn(), small_fcnn(), \"SmallBTCNN_SmallFCNN\"),\n",
    "        BinaryTreeRegressor(small_btcnn(), medium_fcnn(), \"SmallBTCNN_MediumFCNN\"),\n",
    "        BinaryTreeRegressor(medium_btcnn(), small_fcnn(), \"MediumBTCNN_SmallFCNN\"),\n",
    "        BinaryTreeRegressor(medium_btcnn(), medium_fcnn(), \"MediumBTCNN_MediumFCNN\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_btcnn = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, 64),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(64, 128),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(128, 256),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(256, 512),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_btcnn_and_frozen_layer_norm = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, 64),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(64, 128),\n",
    "    BinaryTreeLayerNorm(128, frozen=True),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(128, 256),\n",
    "    BinaryTreeLayerNorm(256, frozen=True),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(256, 512),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_btcnn_and_frozen_instance_norm = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, 64),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(64, 128),\n",
    "    BinaryTreeInstanceNorm(128, frozen=True),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(128, 256),\n",
    "    BinaryTreeInstanceNorm(256, frozen=True),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(256, 512),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_btcnn_and_layer_norm = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, 64),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(64, 128),\n",
    "    BinaryTreeLayerNorm(128),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(128, 256),\n",
    "    BinaryTreeLayerNorm(256),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(256, 512),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_btcnn_and_instance_norm = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, 64),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(64, 128),\n",
    "    BinaryTreeInstanceNorm(128),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(128, 256),\n",
    "    BinaryTreeInstanceNorm(256),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(256, 512),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_fcnn = lambda: torch.nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_big_models():\n",
    "    return [\n",
    "        BinaryTreeRegressor(big_btcnn(), big_fcnn(), \"BigBTCNN_BigFCNN\"),\n",
    "        BinaryTreeRegressor(big_btcnn_and_frozen_layer_norm(), big_fcnn(), \"BigBTCNN_BigFCNN_FrozenLayerNorm\"),\n",
    "        BinaryTreeRegressor(big_btcnn_and_frozen_instance_norm(), big_fcnn(), \"BigBTCNN_BigFCNN_FrozenInstanceNorm\"),\n",
    "        BinaryTreeRegressor(big_btcnn_and_layer_norm(), big_fcnn(), \"BigBTCNN_BigFCNN_LayerNorm\"),\n",
    "        BinaryTreeRegressor(big_btcnn_and_instance_norm(), big_fcnn(), \"BigBTCNN_BigFCNN_InstanceNorm\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([v.shape[0] for v in job_list_vertices + ood_sq_list_vertices + id_sq_list_vertices])\n",
    "print(f\"The longest tree has length {max_length}\")\n",
    "batch_size = 256\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloaders(n):\n",
    "    res = []\n",
    "    for seed in range(42, 42+n):\n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "        train_dataset, val_dataset = torch.utils.data.dataset.random_split(job_weighted_dataset, [0.8, 0.2], generator=generator)\n",
    "        test_dataset = id_sq_weighted_dataset\n",
    "        ood_dataset = ood_sq_weighted_dataset\n",
    "        train_dataloader, val_dataloader, test_dataloader, ood_dataloader = [\n",
    "            DataLoader(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=lambda el: weighted_binary_tree_collate(el, max_length),\n",
    "                drop_last=False\n",
    "            )\n",
    "            for dataset in [train_dataset, val_dataset, test_dataset, ood_dataset]\n",
    "        ]        \n",
    "        yield (train_dataloader, val_dataloader, test_dataloader, ood_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, optimizer, criterion, dataloader, train_mode=True):\n",
    "    model.train() if train_mode else model.eval()\n",
    "    running_loss, total_samples = .0, 0\n",
    "    for (vertices, edges, freq), time in dataloader:\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(vertices, edges)\n",
    "        weighted_loss = (freq.float().squeeze(-1) * criterion(outputs.squeeze(-1), time)).mean()\n",
    "        \n",
    "        if train_mode:\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += weighted_loss.item() * vertices.size(0)\n",
    "        total_samples += freq.sum()\n",
    "    return running_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(model, optimizer, scheduler, epoch, path):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_train_loop(\n",
    "    model,  optimizer,  criterion, scheduler, train_dataloader, num_epochs, clearml_task, \n",
    "    start_epoch=0, metadata=None, ckpt_period=10, eval_period=10, path_to_save=None, val_dataloader=None, test_dataloader=None, ood_dataloader=None\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            train_loss = calculate_loss(model, optimizer, criterion, train_dataloader, train_mode=False)\n",
    "            clearml_task.get_logger().report_scalar(\"MSE [train]\", model.name, iteration=0, value=train_loss)\n",
    "            if val_dataloader:\n",
    "                val_loss = calculate_loss(model, optimizer, criterion, val_dataloader, train_mode=False)\n",
    "                clearml_task.get_logger().report_scalar(\"MSE [val]\", model.name, iteration=0, value=val_loss)\n",
    "            if test_dataloader:\n",
    "                test_loss = calculate_loss(model, optimizer, criterion, test_dataloader, train_mode=False)\n",
    "                clearml_task.get_logger().report_scalar(\"MSE [test]\", model.name, iteration=0, value=test_loss)\n",
    "            if ood_dataloader:  \n",
    "                ood_loss = calculate_loss(model, optimizer, criterion, ood_dataloader, train_mode=False)\n",
    "                clearml_task.get_logger().report_scalar(\"MSE [ood]\", model.name, iteration=0, value=ood_loss)            \n",
    "\n",
    "        tqdm_desc = \"Initialization\"\n",
    "        progress_bar = tqdm(range(start_epoch + 1, start_epoch + num_epochs + 1), desc=tqdm_desc, leave=True, position=0)\n",
    "        for epoch in progress_bar:\n",
    "            train_loss = calculate_loss(model, optimizer, criterion, train_dataloader)\n",
    "            scheduler.step(train_loss)\n",
    "            clearml_task.get_logger().report_scalar(\"MSE [train]\", model.name, iteration=epoch, value=train_loss)\n",
    "            progress_bar.set_description(f'[{epoch}/{start_epoch + num_epochs}] MSE: {train_loss:.4f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if val_dataloader and not epoch % eval_period:\n",
    "                    val_loss = calculate_loss(model, optimizer, criterion, val_dataloader, train_mode=False)\n",
    "                    clearml_task.get_logger().report_scalar(\"MSE [val]\", model.name, iteration=epoch, value=val_loss)\n",
    "                if test_dataloader and not epoch % eval_period:\n",
    "                    test_loss = calculate_loss(model, optimizer, criterion, test_dataloader, train_mode=False)\n",
    "                    clearml_task.get_logger().report_scalar(\"MSE [test]\", model.name,iteration=epoch, value=test_loss)\n",
    "                if ood_dataloader and not epoch % eval_period:  \n",
    "                    ood_loss = calculate_loss(model, optimizer, criterion, ood_dataloader, train_mode=False)\n",
    "                    clearml_task.get_logger().report_scalar(\"MSE [ood]\", model.name,iteration=epoch, value=ood_loss)            \n",
    "\n",
    "            if path_to_save and not epoch % ckpt_period:\n",
    "                save_ckpt(model, optimizer, scheduler, epoch, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/allegroai/clearml\n",
    "!pip install clearml-agent\n",
    "from clearml import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task.init(project_name=\"hero\", task_name='Medium Models')\n",
    "assert task is not None\n",
    "\n",
    "epochs = 300\n",
    "metadata = {\n",
    "    \"data\": \"weighted_dataset\",\n",
    "    \"lr\": lr,\n",
    "    \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "n_runs = 5\n",
    "for run, (train_dataloader, val_dataloader, test_dataloader, ood_dataloader) in enumerate(generate_dataloaders(n_runs), start=1):\n",
    "    for model in initialize_models():\n",
    "        model.name = model.name + \"_\" + str(run)\n",
    "        model.btcnn.to(DEVICE)\n",
    "        model.fcnn.to(DEVICE)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "        set_seed(2024)\n",
    "        weighted_train_loop(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=nn.MSELoss(reduction=\"none\"),\n",
    "            scheduler=scheduler,\n",
    "            train_dataloader=train_dataloader,\n",
    "            num_epochs=epochs,\n",
    "            clearml_task=task,\n",
    "            metadata=metadata,\n",
    "            ckpt_period=epochs,\n",
    "            eval_period=5,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            ood_dataloader=ood_dataloader,\n",
    "            path_to_save=f\"{EXPERIMENT_PATH}/models/{model.name}.pth\",\n",
    "        )\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task.init(project_name=\"hero\", task_name='Big Models')\n",
    "assert task is not None\n",
    "\n",
    "epochs = 300\n",
    "metadata = {\n",
    "    \"data\": \"weighted_dataset\",\n",
    "    \"lr\": lr,\n",
    "    \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "n_runs = 5\n",
    "for run, (train_dataloader, val_dataloader, test_dataloader, ood_dataloader) in enumerate(generate_dataloaders(n_runs), start=1):\n",
    "    for model in initialize_big_models():\n",
    "        model.name = model.name + \"_\" + str(run)\n",
    "        model.btcnn.to(DEVICE)\n",
    "        model.fcnn.to(DEVICE)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "        set_seed(2024)\n",
    "        weighted_train_loop(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=nn.MSELoss(reduction=\"none\"),\n",
    "            scheduler=scheduler,\n",
    "            train_dataloader=train_dataloader,\n",
    "            num_epochs=epochs,\n",
    "            clearml_task=task,\n",
    "            metadata=metadata,\n",
    "            ckpt_period=epochs,\n",
    "            eval_period=5,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            ood_dataloader=ood_dataloader,\n",
    "            path_to_save=f\"{EXPERIMENT_PATH}/models/{model.name}.pth\",\n",
    "        )\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noised Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we investigate the role of operation channels and statistics in vertex embedding. The goal is to understand whether tree structure alone or statistics alone is sufficient to predict time with equal accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_vertices(v, channels, generator):\n",
    "    noise = torch.rand(v.shape, device=v.device, generator=generator)\n",
    "    res = v.clone()\n",
    "    res[:,channels] = noise[:,channels]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisedWeightedBinaryTreeDataset(WeightedBinaryTreeDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        list_vertices: \"List[Tensor]\",\n",
    "        list_edges: \"List[Tensor]\",\n",
    "        list_time: \"List[Tensor]\",\n",
    "        device: \"torch.device\",\n",
    "        noised_channels: \"List[int]\",  \n",
    "        generator: \"torch.Generator\",\n",
    "    ):\n",
    "        super().__init__(list_vertices, list_edges, list_time, device)\n",
    "        \n",
    "        self.noised_channels = noised_channels\n",
    "        for i, v in enumerate(self.list_vertices):\n",
    "            self.list_vertices[i] = add_noise_to_vertices(v, channels=self.noised_channels, generator=generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noised_dataloaders(n, noised_channels):\n",
    "    res = []\n",
    "    for seed in range(42, 42+n):\n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "        noised_trainval_dataset = NoisedWeightedBinaryTreeDataset(job_list_vertices, job_list_edges, job_list_time, DEVICE, noised_channels, generator)\n",
    "        noised_test_dataset = NoisedWeightedBinaryTreeDataset(id_sq_list_vertices, id_sq_list_edges, id_sq_list_time, DEVICE, noised_channels, generator)\n",
    "        noised_ood_dataset = NoisedWeightedBinaryTreeDataset(ood_sq_list_vertices, ood_sq_list_edges, ood_sq_list_time, DEVICE, noised_channels, generator)\n",
    "    \n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "        noised_train_dataset, noised_val_dataset = torch.utils.data.dataset.random_split(noised_trainval_dataset, [0.8, 0.2], generator=generator)\n",
    "\n",
    "        train_dataloader, val_dataloader, test_dataloader, ood_dataloader = [\n",
    "            DataLoader(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=lambda el: weighted_binary_tree_collate(el, max_length),\n",
    "                drop_last=False\n",
    "            )\n",
    "            for dataset in [noised_train_dataset, noised_val_dataset, noised_test_dataset, noised_ood_dataset]\n",
    "        ]        \n",
    "        yield (train_dataloader, val_dataloader, test_dataloader, ood_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task.init(project_name=\"hero\", task_name=\"Noised data\")\n",
    "assert task is not None\n",
    "\n",
    "epochs = 150\n",
    "metadata = {\n",
    "    \"data\": \"noised_weighted_dataset\",\n",
    "    \"lr\": lr,\n",
    "    \"batch_size\": batch_size\n",
    "}\n",
    "n_runs = 10\n",
    "\n",
    "for noised_channels, noise_description in [\n",
    "    (range(len(ALL_FEATURES)), \"all\"), \n",
    "    (range(len(ALL_FEATURES)-2), \"ops\"), \n",
    "    (range(len(ALL_FEATURES)-2, len(ALL_FEATURES)), \"stats\"),\n",
    "    ]:\n",
    "    for run, (train_dataloader, val_dataloader, test_dataloader, ood_dataloader) in enumerate(generate_noised_dataloaders(n_runs, noised_channels), start=1):\n",
    "        model = BinaryTreeRegressor(big_btcnn_and_instance_norm(), big_fcnn(), \"BigBTCNN_BigFCNN_InstanceNorm\")\n",
    "        model.name = \"[\" + noise_description + \"] \" + model.name + \"_\" + str(run)\n",
    "        model.btcnn.to(DEVICE)\n",
    "        model.fcnn.to(DEVICE)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "        set_seed(2024)\n",
    "        weighted_train_loop(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=nn.MSELoss(reduction=\"none\"),\n",
    "            scheduler=scheduler,\n",
    "            train_dataloader=train_dataloader,\n",
    "            num_epochs=epochs,\n",
    "            clearml_task=task,\n",
    "            metadata=metadata,\n",
    "            ckpt_period=epochs,\n",
    "            eval_period=5,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            ood_dataloader=ood_dataloader,\n",
    "            path_to_save=f\"{EXPERIMENT_PATH}/models/{model.name}.pth\",\n",
    "        )\n",
    "\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
