{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook's outline:**\n",
    "\n",
    "- load data for `JOB` benchmark and check the degree of uniqueness in it!\n",
    "- proof the usefullness of tree convolution\n",
    "- build neural network with best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "  from google.colab import drive\n",
    "  print(\"Hello, Colab\")\n",
    "  drive.mount('/content/drive')\n",
    "  ROOT_PATH = \"/content/drive/MyDrive/hero\"\n",
    "  os.environ['CLEARML_CONFIG_FILE'] = f'{ROOT_PATH}/clearml.conf'\n",
    "else:\n",
    "  ROOT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "EXPERIMENT_PATH = f\"{ROOT_PATH}/experiments/tcnn-abilities\"\n",
    "sys.path.insert(0, ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from json import load, dumps, dump\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from src.utils import get_logical_plan, get_full_plan, get_selectivities\n",
    "from src.models import binary_tree_layers as btl\n",
    "from src.datasets.oracle import Oracle, OracleRequest, TIMEOUT\n",
    "from src.datasets.data_config import HINTSETS, DOPS, HINTS, DEFAULT_HINTSET\n",
    "from src.datasets.data_types import ExplainNode\n",
    "from src.datasets.vectorization import extract_vertices_and_edges, ALL_FEATURES\n",
    "from src.datasets.binary_tree_dataset import binary_tree_collate, BinaryTreeDataset, WeightedBinaryTreeDataset, weighted_binary_tree_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавь описание того, почему sample queries стал test, и что такое OOD, как это мерилось и так далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_oracle = Oracle(f\"{ROOT_PATH}/data/processed/JOB\")\n",
    "sq_oracle = Oracle(f\"{ROOT_PATH}/data/processed/sample_queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_info(oracle, query_names):\n",
    "    \"\"\" Collects dicts with info 'query_name', 'hintset', 'dop', 'vertices', 'edges', 'time'\"\"\"\n",
    "    list_info = []\n",
    "\n",
    "    for query_name in tqdm(query_names, leave=False):\n",
    "        seen_logical_plans = set()\n",
    "        timeouted_logical_plans_to_dops = defaultdict(set)\n",
    "        timeouted_logical_plans_to_settings = defaultdict(list)\n",
    "        logical_plan_to_times = defaultdict(list)\n",
    "        for dop in DOPS:            \n",
    "            for hintset in HINTSETS:\n",
    "                custom_request = OracleRequest(query_name=query_name, hintset=hintset,dop=dop)\n",
    "                custom_logical_plan = get_logical_plan(\n",
    "                    query_name=query_name, \n",
    "                    oracle=oracle, \n",
    "                    hintset=hintset, \n",
    "                    dop=dop\n",
    "                )\n",
    "                 \n",
    "                custom_time = oracle.get_execution_time(custom_request)\n",
    "                if custom_time != TIMEOUT:\n",
    "                    info = {\"query_name\": query_name, \"hintset\": hintset, \"dop\": dop}\n",
    "                    time = torch.tensor(custom_time / 1000, dtype=torch.float32)\n",
    "                    vertices, edges = extract_vertices_and_edges(\n",
    "                        oracle.get_explain_plan(request=custom_request)\n",
    "                    )\n",
    "                    info.update({\"time\": time, \"vertices\": vertices, \"edges\": edges}) \n",
    "                    seen_logical_plans.add(custom_logical_plan)\n",
    "                    list_info.append(info)\n",
    "                    logical_plan_to_times[custom_logical_plan].append(time)\n",
    "                else:\n",
    "                    timeouted_logical_plans_to_dops[custom_logical_plan].add(dop)\n",
    "                    timeouted_logical_plans_to_settings[custom_logical_plan].append((dop, hintset))\n",
    "\n",
    "        for custom_logical_plan in timeouted_logical_plans_to_settings:             \n",
    "            if custom_logical_plan in logical_plan_to_times:\n",
    "                time = sum(logical_plan_to_times[custom_logical_plan]) / len(logical_plan_to_times[custom_logical_plan])\n",
    "            else:\n",
    "                max_def_time = 0\n",
    "                for dop in timeouted_logical_plans_to_dops[custom_logical_plan]:\n",
    "                    def_request = OracleRequest(query_name=query_name, hintset=0, dop=dop)\n",
    "                    def_time = oracle.get_execution_time(request=def_request)\n",
    "                    max_def_time = max(max_def_time, def_time)\n",
    "                time = torch.tensor(2 * max_def_time / 1000, dtype=torch.float32)\n",
    "\n",
    "            for dop, hintset in timeouted_logical_plans_to_settings[custom_logical_plan]:\n",
    "                info = {\"query_name\": query_name, \"hintset\": hintset, \"dop\": dop}\n",
    "                custom_request = OracleRequest(query_name=query_name, hintset=hintset, dop=dop)\n",
    "                vertices, edges = extract_vertices_and_edges(\n",
    "                    oracle.get_explain_plan(request=custom_request)\n",
    "                )\n",
    "                info.update({\"time\": time, \"vertices\": vertices, \"edges\": edges})             \n",
    "                list_info.append(info)\n",
    "            \n",
    "    return list_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB]: dataset size is 43392 / 43392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "job_list_info = extract_list_info(oracle=job_oracle, query_names=job_oracle.get_query_names())\n",
    "job_max_possible_size = len(job_oracle.get_query_names()) * len(DOPS) * len(HINTSETS)\n",
    "print(f\"[JOB]: dataset size is {len(job_list_info)} / {job_max_possible_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list_vertices = [info[\"vertices\"] for info in job_list_info]\n",
    "job_list_edges = [info[\"edges\"] for info in job_list_info]\n",
    "job_list_time = [info[\"time\"] for info in job_list_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SQ]: dataset size is 15360 / 15360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "sq_list_info = extract_list_info(oracle=sq_oracle, query_names=sq_oracle.get_query_names())\n",
    "sq_max_possible_size = len(sq_oracle.get_query_names()) * len(DOPS) * len(HINTSETS)\n",
    "print(f\"[SQ]: dataset size is {len(sq_list_info)} / {sq_max_possible_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_list_vertices = [info[\"vertices\"] for info in sq_list_info]\n",
    "sq_list_edges = [info[\"edges\"] for info in sq_list_info]\n",
    "sq_list_time = [info[\"time\"] for info in sq_list_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around 23.0 % of plans from SQ bench exists in JOB bench\n"
     ]
    }
   ],
   "source": [
    "job_X = set([(str(v.flatten().tolist()), str(e.flatten().tolist())) for v, e in zip(job_list_vertices, job_list_edges)])\n",
    "sq_X = set([(str(v.flatten().tolist()), str(e.flatten().tolist())) for v, e in zip(sq_list_vertices, sq_list_edges)])\n",
    "print(f\"Around {100 * len(sq_X & job_X) / len(sq_X):0.1f} % of plans from SQ bench exists in JOB bench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_logical_plans = set(\n",
    "    get_logical_plan(oracle=job_oracle,  query_name=query_name, hintset=hintset, dop=dop)\n",
    "    for query_name in job_oracle.get_query_names() for hintset in HINTSETS for dop in DOPS\n",
    ")\n",
    "\n",
    "ood_sq_list_info, id_sq_list_info = [], []\n",
    "for info in sq_list_info:\n",
    "    logical_plan = get_logical_plan(oracle=sq_oracle, query_name=info[\"query_name\"], hintset=info[\"hintset\"], dop=info[\"dop\"])\n",
    "    if logical_plan not in job_logical_plans:\n",
    "        ood_sq_list_info.append(info)\n",
    "    else:\n",
    "        id_sq_list_info.append(info)\n",
    "assert len(ood_sq_list_info) + len(id_sq_list_info) == len(sq_list_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_sq_list_vertices = [info[\"vertices\"] for info in ood_sq_list_info]\n",
    "ood_sq_list_edges = [info[\"edges\"] for info in ood_sq_list_info]\n",
    "ood_sq_list_time = [info[\"time\"] for info in ood_sq_list_info]\n",
    "\n",
    "id_sq_list_vertices = [info[\"vertices\"] for info in id_sq_list_info]\n",
    "id_sq_list_edges = [info[\"edges\"] for info in id_sq_list_info]\n",
    "id_sq_list_time = [info[\"time\"] for info in id_sq_list_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB]: total # of unique plans: 7429 / 43392\n"
     ]
    }
   ],
   "source": [
    "job_all_plans = [\n",
    "    get_full_plan(oracle=job_oracle, query_name=query_name, hintset=hintset, dop=dop)\n",
    "    for query_name in job_oracle.get_query_names() for hintset in HINTSETS for dop in DOPS\n",
    "]\n",
    "print(f\"[JOB]: total # of unique plans: {len(set(job_all_plans))} / {len(job_all_plans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SQ]: total # of unique plans: 4496 / 15360\n"
     ]
    }
   ],
   "source": [
    "sq_all_plans = [\n",
    "    get_full_plan(oracle=sq_oracle, query_name=query_name, hintset=hintset, dop=dop)\n",
    "    for query_name in sq_oracle.get_query_names() for hintset in HINTSETS for dop in DOPS\n",
    "]\n",
    "print(f\"[SQ]: total # of unique plans: {len(set(sq_all_plans))} / {len(sq_all_plans)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation.** We see that most of the plans are repetitive. This is the reason for switching to *weighted datasets* to speed up the learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB]: total # of unique plans in the weighted dataset 7323\n"
     ]
    }
   ],
   "source": [
    "job_dataset = BinaryTreeDataset(job_list_vertices, job_list_edges, job_list_time, DEVICE)\n",
    "job_weighted_dataset = WeightedBinaryTreeDataset(job_list_vertices, job_list_edges, job_list_time, DEVICE)\n",
    "print(f\"[JOB]: total # of unique plans in the weighted dataset {len(job_weighted_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SQ, OOD]: total # of unique plans in the weighted dataset 3409\n",
      "[SQ, ID]: total # of unique plans in the weighted dataset 1021\n"
     ]
    }
   ],
   "source": [
    "ood_sq_dataset = BinaryTreeDataset(ood_sq_list_vertices, ood_sq_list_edges, ood_sq_list_time, DEVICE)\n",
    "ood_sq_weighted_dataset = WeightedBinaryTreeDataset(ood_sq_list_vertices, ood_sq_list_edges, ood_sq_list_time, DEVICE)\n",
    "print(f\"[SQ, OOD]: total # of unique plans in the weighted dataset {len(ood_sq_weighted_dataset)}\")\n",
    "\n",
    "id_sq_dataset = BinaryTreeDataset(id_sq_list_vertices, id_sq_list_edges, id_sq_list_time, DEVICE)\n",
    "id_sq_weighted_dataset = WeightedBinaryTreeDataset(id_sq_list_vertices, id_sq_list_edges, id_sq_list_time, DEVICE)\n",
    "print(f\"[SQ, ID]: total # of unique plans in the weighted dataset {len(id_sq_weighted_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O**ut **O**f **D**istribution vs **I**n **D**istribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_best_predictor(dataset):\n",
    "    sum_dict = defaultdict(float)\n",
    "    count_dict = defaultdict(int)\n",
    "\n",
    "    for v, e, t in dataset:\n",
    "        key = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "        sum_dict[key] += t.item()\n",
    "        count_dict[key] += 1\n",
    "\n",
    "    def predictor(v, e):\n",
    "        key = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "        if key not in sum_dict:\n",
    "            return None\n",
    "        return sum_dict[key] / count_dict[key]\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = build_best_predictor(job_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of data from JOB, mse / best possible mse ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the worst and best possible predictors so we know what we should aim for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse_for_best_constant_predictor(dataset):\n",
    "    time_sum = 0\n",
    "    count = 0\n",
    "\n",
    "    for v, e, t in dataset:\n",
    "        time_sum += t\n",
    "        count += 1\n",
    "\n",
    "    mse_loss = 0\n",
    "    for v, e, t in dataset:\n",
    "        t_pred = time_sum / count\n",
    "        mse_loss += (t_pred - t) ** 2\n",
    "\n",
    "    return mse_loss.item() / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse_for_best_possible_predictor(dataset):\n",
    "    sum_dict = defaultdict(float)\n",
    "    count_dict = defaultdict(int)\n",
    "\n",
    "    for v, e, t in dataset:\n",
    "        key = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "        sum_dict[key] += t.item()\n",
    "        count_dict[key] += 1\n",
    "\n",
    "    mse_loss = 0\n",
    "    for v, e, t in dataset:\n",
    "        key = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "        t_pred = sum_dict[key] / count_dict[key]\n",
    "        mse_loss += (t_pred - t) ** 2\n",
    "\n",
    "    return mse_loss/ len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB]: MSE of the best constant predictor is 135.607\n",
      "[JOB]: MSE of the best possible predictor is 0.662\n"
     ]
    }
   ],
   "source": [
    "print(f\"[JOB]: MSE of the best constant predictor is {calculate_mse_for_best_constant_predictor(job_dataset):0.3f}\")\n",
    "print(f\"[JOB]: MSE of the best possible predictor is {calculate_mse_for_best_possible_predictor(job_dataset):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SQ, OOD]: MSE of the best constant predictor is 3953.941\n",
      "[SQ, OOD]: MSE of the best possible predictor is 0.036\n",
      "[SQ, ID]: MSE of the best constant predictor is 650.152\n",
      "[SQ, ID]: MSE of the best possible predictor is 0.105\n"
     ]
    }
   ],
   "source": [
    "print(f\"[SQ, OOD]: MSE of the best constant predictor is {calculate_mse_for_best_constant_predictor(ood_sq_dataset):0.3f}\")\n",
    "print(f\"[SQ, OOD]: MSE of the best possible predictor is {calculate_mse_for_best_possible_predictor(ood_sq_dataset):0.3f}\")\n",
    "\n",
    "print(f\"[SQ, ID]: MSE of the best constant predictor is {calculate_mse_for_best_constant_predictor(id_sq_dataset):0.3f}\")\n",
    "print(f\"[SQ, ID]: MSE of the best possible predictor is {calculate_mse_for_best_possible_predictor(id_sq_dataset):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(model, optimizer, scheduler, epoch, path):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(model, ckpt_path):\n",
    "    ckpt_state = torch.load(ckpt_path)\n",
    "    model.load_state_dict(ckpt_state['model_state_dict'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer.load_state_dict(ckpt_state['optimizer_state_dict'])\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "    scheduler.load_state_dict(ckpt_state['scheduler_state_dict'])\n",
    "    start_epoch = ckpt_state[\"epoch\"]    \n",
    "    return model, optimizer, scheduler, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, q_n, hs, dop):\n",
    "    request = OracleRequest(query_name=q_n, hintset=hs, dop=dop)\n",
    "    explain_plan = oracle.get_explain_plan(request=request)\n",
    "    vertices, edges = extract_vertices_and_edges(explain_plan)\n",
    "    (vertices_batch, edges_batch), y_batch = binary_tree_collate([(vertices, edges, torch.tensor([1.0]))], max_length)                \n",
    "    return float(model(vertices_batch, edges_batch).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(oracle, query_name, hs, dop):\n",
    "    request = OracleRequest(query_name=query_name, hintset=hs, dop=dop)\n",
    "    explain_plan = oracle.get_execution_time(request=request)\n",
    "    return explain_plan / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    loss_sum = .0\n",
    "    for ve, t in tqdm(dataloader):\n",
    "        loss_sum += sum((model(*ve).squeeze(-1) - t).to(\"cpu\") ** 2)\n",
    "    print(f'MSE of given model is {loss_sum.item() / len(dataloader.dataset):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = len(ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTreeRegressor(nn.Module):\n",
    "    def __init__(self, btcnn: \"btl.BinaryTreeSequential\", fcnn: \"torch.nn.Sequential\", name: \"str\" = \"unknown\"):\n",
    "        super().__init__()\n",
    "        self.btcnn: \"btl.BinaryTreeSequential\" = btcnn\n",
    "        self.fcnn: \"torch.nn.Sequential\" = fcnn\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, vertices: \"Tensor\", edges: \"Tensor\") -> \"Tensor\":\n",
    "        return self.fcnn(self.btcnn(vertices=vertices, edges=edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The usefullness of tree convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_btcnn = lambda: btl.BinaryTreeSequential(\n",
    "    btl.BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "small_btcnn = lambda: btl.BinaryTreeSequential(\n",
    "    btl.BinaryTreeConv(in_channels, in_channels),\n",
    "    btl.BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "medium_btcnn = lambda: btl.BinaryTreeSequential(\n",
    "    btl.BinaryTreeConv(in_channels, 128),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(128, in_channels),\n",
    "    btl.BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "small_fcnn = lambda: torch.nn.Sequential(\n",
    "    nn.Linear(in_channels, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ")\n",
    "\n",
    "medium_fcnn = lambda: torch.nn.Sequential(\n",
    "    nn.Linear(in_channels, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    return [\n",
    "        BinaryTreeRegressor(no_btcnn(), small_fcnn(), \"NoBTCNN_SmallFCNN\"),\n",
    "        BinaryTreeRegressor(no_btcnn(), medium_fcnn(), \"NoBTCNN_MediumFCNN\"),\n",
    "        BinaryTreeRegressor(small_btcnn(), small_fcnn(), \"SmallBTCNN_SmallFCNN\"),\n",
    "        BinaryTreeRegressor(small_btcnn(), medium_fcnn(), \"SmallBTCNN_MediumFCNN\"),\n",
    "        BinaryTreeRegressor(medium_btcnn(), small_fcnn(), \"MediumBTCNN_SmallFCNN\"),\n",
    "        BinaryTreeRegressor(medium_btcnn(), medium_fcnn(), \"MediumBTCNN_MediumFCNN\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest tree has length 66\n"
     ]
    }
   ],
   "source": [
    "max_length = max([v.shape[0] for v in job_list_vertices + ood_sq_list_vertices + id_sq_list_vertices])\n",
    "print(f\"The longest tree has length {max_length}\")\n",
    "batch_size = 32\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloaders(n):\n",
    "    res = []\n",
    "    for seed in range(42, 42+n):\n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "        train_dataset, val_dataset = torch.utils.data.dataset.random_split(job_weighted_dataset, [0.8, 0.2], generator=generator)\n",
    "        test_dataset = id_sq_weighted_dataset\n",
    "        ood_dataset = ood_sq_weighted_dataset\n",
    "        train_dataloader, val_dataloader, test_dataloader, ood_dataloader = [\n",
    "            DataLoader(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=lambda el: weighted_binary_tree_collate(el, max_length),\n",
    "                drop_last=False\n",
    "            )\n",
    "            for dataset in [train_dataset, val_dataset, test_dataset, ood_dataset]\n",
    "        ]        \n",
    "        yield (train_dataloader, val_dataloader, test_dataloader, ood_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, optimizer, criterion, dataloader, train_mode=True):\n",
    "    model.train() if train_mode else model.eval()\n",
    "    running_loss, total_samples = .0, 0\n",
    "    for (vertices, edges, freq), time in dataloader:\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(vertices, edges)\n",
    "        weighted_loss = (freq.float().squeeze(-1) * criterion(outputs.squeeze(-1), time)).mean()\n",
    "        \n",
    "        if train_mode:\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += weighted_loss.item() * vertices.size(0)\n",
    "        total_samples += freq.sum()\n",
    "    return running_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_train_loop(\n",
    "    model,  optimizer,  criterion, scheduler, train_dataloader, num_epochs, clearml_task, \n",
    "    start_epoch=0, metadata=None, ckpt_period=10, eval_period=10, path_to_save=None, val_dataloader=None, test_dataloader=None, ood_dataloader=None\n",
    "    ):\n",
    "    tqdm_desc = \"Initialization\"\n",
    "    progress_bar = tqdm(range(start_epoch + 1, start_epoch + num_epochs + 1), desc=tqdm_desc, leave=True, position=0)\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        train_loss = calculate_loss(model, optimizer, criterion, train_dataloader)\n",
    "        scheduler.step(train_loss)\n",
    "        clearml_task.get_logger().report_scalar('MSE', \"[train] \" + model.name, iteration=epoch, value=train_loss)\n",
    "        progress_bar.set_description(f'[{epoch}/{start_epoch + num_epochs}] MSE: {train_loss:.4f}')\n",
    "\n",
    "        if val_dataloader and not epoch % eval_period:\n",
    "          with torch.no_grad():\n",
    "            val_loss = calculate_loss(model, optimizer, criterion, val_dataloader, train_mode=False)\n",
    "            clearml_task.get_logger().report_scalar('MSE', \"[val] \" + model.name, iteration=epoch, value=val_loss)\n",
    "\n",
    "        if test_dataloader and not epoch % eval_period:\n",
    "          with torch.no_grad():\n",
    "            test_loss = calculate_loss(model, optimizer, criterion, test_dataloader, train_mode=False)\n",
    "            clearml_task.get_logger().report_scalar('MSE', \"[test] \" + model.name,iteration=epoch, value=test_loss)\n",
    "\n",
    "        if ood_dataloader and not epoch % eval_period:\n",
    "            with torch.no_grad():\n",
    "              ood_loss = calculate_loss(model, optimizer, criterion, ood_dataloader, train_mode=False)\n",
    "              clearml_task.get_logger().report_scalar('MSE', \"[ood] \" + model.name,iteration=epoch, value=ood_loss)            \n",
    "\n",
    "        if path_to_save and not epoch % ckpt_period:\n",
    "            save_ckpt(model, optimizer, scheduler, epoch, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/allegroai/clearml\n",
    "!pip install clearml-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=3ce65a095d204e5e98eb410780f12ad3\n",
      "2024-07-01 16:01:35,316 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/8c90433626c94a93bf97392993994c51/experiments/3ce65a095d204e5e98eb410780f12ad3/output/log\n"
     ]
    }
   ],
   "source": [
    "from clearml import Task\n",
    "task_for_tcnn = Task.init(project_name=\"hero\", task_name='Test generalization')\n",
    "assert task_for_tcnn is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/50] MSE: 124.1291:   2%|▏         | 1/50 [00:08<07:17,  8.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     17\u001b[0m set_seed(\u001b[38;5;241m2024\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mweighted_train_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclearml_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_for_tcnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mood_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mood_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mEXPERIMENT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m, in \u001b[0;36mweighted_train_loop\u001b[0;34m(model, optimizer, criterion, scheduler, train_dataloader, num_epochs, clearml_task, start_epoch, metadata, ckpt_period, eval_period, path_to_save, val_dataloader, test_dataloader, ood_dataloader)\u001b[0m\n\u001b[1;32m      6\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, start_epoch \u001b[38;5;241m+\u001b[39m num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39mtqdm_desc, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m----> 9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[1;32m     11\u001b[0m     clearml_task\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mreport_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[train] \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mname, iteration\u001b[38;5;241m=\u001b[39mepoch, value\u001b[38;5;241m=\u001b[39mtrain_loss)\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(model, optimizer, criterion, dataloader, train_mode)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[1;32m     12\u001b[0m     weighted_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 13\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weighted_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m vertices\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m freq\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/GitHub/hero/venv/lib/python3.9/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/hero/venv/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/GitHub/hero/venv/lib/python3.9/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/GitHub/hero/venv/lib/python3.9/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/hero/venv/lib/python3.9/site-packages/torch/optim/adam.py:394\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 394\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    397\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "metadata = {\n",
    "    \"data\": \"weighted_dataset\",\n",
    "    \"lr\": lr,\n",
    "    \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "n_runs = 5\n",
    "for run, (train_dataloader, val_dataloader, test_dataloader, ood_dataloader) in enumerate(generate_dataloaders(n_runs), start=1):\n",
    "    for model in initialize_models():\n",
    "        model.name = model.name + \"_\" + str(run)\n",
    "        model.btcnn.to(DEVICE)\n",
    "        model.fcnn.to(DEVICE)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "        set_seed(2024)\n",
    "        weighted_train_loop(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=nn.MSELoss(reduction=\"none\"),\n",
    "            scheduler=scheduler,\n",
    "            train_dataloader=train_dataloader,\n",
    "            num_epochs=epochs,\n",
    "            clearml_task=task_for_tcnn,\n",
    "            metadata=metadata,\n",
    "            ckpt_period=300,\n",
    "            eval_period=5,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            ood_dataloader=ood_dataloader,\n",
    "            path_to_save=f\"{EXPERIMENT_PATH}/models/{model.name}.pth\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_btcnn = lambda: btl.BinaryTreeSequential(\n",
    "    btl.BinaryTreeConv(in_channels, 64),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(64, 128),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(128, 256),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(256, 512),\n",
    "    btl.BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_btcnn_and_layer_norm = lambda: btl.BinaryTreeSequential(\n",
    "    btl.BinaryTreeConv(in_channels, 64),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(64, 128),\n",
    "    btl.BinaryTreeLayerNorm(128),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(128, 256),\n",
    "    btl.BinaryTreeLayerNorm(256),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(256, 512),\n",
    "    btl.BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_btcnn_and_instance_norm = lambda: btl.BinaryTreeSequential(\n",
    "    btl.BinaryTreeConv(in_channels, 64),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(64, 128),\n",
    "    btl.BinaryTreeInstanceNorm(128),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(128, 256),\n",
    "    btl.BinaryTreeInstanceNorm(256),\n",
    "    btl.BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    btl.BinaryTreeConv(256, 512),\n",
    "    btl.BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_fcnn = lambda: torch.nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ")\n",
    "\n",
    "def initialize_big_models():\n",
    "    return [\n",
    "        BinaryTreeRegressor(big_btcnn(), big_fcnn(), \"BigBTCNN_BigFCNN\"),\n",
    "        BinaryTreeRegressor(big_btcnn_and_layer_norm(), big_fcnn(), \"BigBTCNN_BigFCNN_LayerNorm\"),\n",
    "        BinaryTreeRegressor(big_btcnn_and_instance_norm(), big_fcnn(), \"BigBTCNN_BigFCNN_InstanceNorm\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-10 19:58:28,200 - clearml.model - INFO - Selected model id: 7e95f3e08d0f43a9b9dfab65748e78b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1356/1356 [00:02<00:00, 612.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of given model is 120.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, scheduler, start_epoch = load_ckpt(\n",
    "    model=BinaryTreeRegressor(no_btcnn(), small_fcnn()),\n",
    "    ckpt_path=f\"{EXPERIMENT_PATH}/models/NoBTCNN_SmallFCNN.pth\",\n",
    ")\n",
    "evaluate_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_on_mse(model, list_info):\n",
    "    total_mse = .0\n",
    "    for info in list_info:\n",
    "        time, query_name, hintset, dop, vertices, edges = info[\"time\"], info[\"query_name\"], info[\"hintset\"], info[\"dop\"], info[\"vertices\"], info[\"edges\"]\n",
    "        prediction = get_prediction(model, query_name, hintset, dop)\n",
    "        total_mse += (prediction - time) ** 2\n",
    "    print(f\"Calculated by hand MSE {total_mse / len(list_info):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated by hand MSE 120.310\n"
     ]
    }
   ],
   "source": [
    "hand_on_mse(model, list_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_on_weighted_mse(model, freq_dataloader):\n",
    "    loss_sum = .0\n",
    "    for (v, e, w), t in tqdm(freq_dataloader):\n",
    "        loss_sum += w.squeeze(-1).to(\"cpu\").float() @ ((model(v, e).squeeze(-1) - t).to(\"cpu\") ** 2)\n",
    "    print(f'MSE of given model is {loss_sum.item() / len(dataloader.dataset):0.3f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228/228 [00:00<00:00, 676.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of given model is 119.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hand_on_weighted_mse(model, weighted_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(model, metadata, ckpt_period, num_epochs, data_loader):\n",
    "    model, optimizer, scheduler, start_epoch = load_ckpt(\n",
    "        model=model,\n",
    "        ckpt_path=f\"{EXPERIMENT_PATH}/models/{model.name}.pth\",\n",
    "    )\n",
    "\n",
    "    model.btcnn.to(DEVICE)\n",
    "    model.fcnn.to(DEVICE)\n",
    "\n",
    "    freq_train_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion = nn.MSELoss(reduction=\"none\"),\n",
    "        scheduler=scheduler,\n",
    "        data_loader=data_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        start_epoch=start_epoch,\n",
    "        clearml_task=task_for_tcnn,\n",
    "        descr=\"[all_plans] \" + name,\n",
    "        metadata=metadata,\n",
    "        ckpt_period=ckpt_period,\n",
    "        path_to_save=f\"{EXPERIMENT_PATH}/models/{name}\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
