{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Paperspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "  from google.colab import drive\n",
    "  print(\"Hello, Colab\")\n",
    "  drive.mount(\"/content/drive\")\n",
    "  ROOT_PATH = \"/content/drive/MyDrive/hero\"\n",
    "  os.environ[\"CLEARML_CONFIG_FILE\"] = f\"{ROOT_PATH}/clearml.conf\"\n",
    "elif \"PAPERSPACE_CLUSTER_ID\" in os.environ:\n",
    "  print(\"Hello, Paperspace\")\n",
    "  ROOT_PATH = \"/notebooks/hero\"\n",
    "  os.environ[\"CLEARML_CONFIG_FILE\"] = f\"{ROOT_PATH}/clearml.conf\"\n",
    "else:\n",
    "  print(\"Hello, Local PC\")\n",
    "  ROOT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "BTCNN_PATH = os.path.join(ROOT_PATH, \"btcnn\")\n",
    "HBO_BENCH_PATH = os.path.join(ROOT_PATH, \"hbo_bench\")\n",
    "\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "sys.path.insert(0, BTCNN_PATH)\n",
    "sys.path.insert(0, HBO_BENCH_PATH)\n",
    "\n",
    "EXPERIMENT_PATH = f\"{ROOT_PATH}/experiments/emulation\"\n",
    "ARTIFACTS_PATH = f\"{EXPERIMENT_PATH}/artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from json import dump, load\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hero import Hero\n",
    "from wrappers import ORACLES_DICT, initialize_oracles, _get_e2e_time, _get_execution_time, _get_planning_time, _get_logical_tree\n",
    "from neural_network import NN\n",
    "from hbo_bench.local_search_settings import *\n",
    "from neural_network import get_bt_regressor\n",
    "from train_utils import load_model\n",
    "from emulation import get_report, emulate_online_learning\n",
    "from hbo_bench.data_config import DEFAULT_DOP, DEFAULT_HINTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_oracles(HBO_BENCH_PATH, [\"JOB\", \"sample_queries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_workload = ORACLES_DICT[\"JOB\"].get_query_names()\n",
    "sq_workload = ORACLES_DICT[\"sample_queries\"].get_query_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideal Case (all plans are in train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[500/500] MSE: 0.0308: 100%|██████████| 500/500 [13:34<00:00,  1.63s/it]\n",
      "[500/500] MSE: 0.5229: 100%|██████████| 500/500 [08:45<00:00,  1.05s/it]  \n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "job_ideal_model = NN(\n",
    "    fit_settings=ALL_SS, \n",
    "    inference_settings=EMPTY_SS, \n",
    "    model=get_bt_regressor(\"job_ideal_model\", DEVICE),\n",
    "    path_to_save=f\"{EXPERIMENT_PATH}/models/job_ideal_model.pth\"\n",
    ")\n",
    "job_ideal_model.fit(job_workload, epochs=epochs)\n",
    "job_ideal_model = load_model(DEVICE, f\"{EXPERIMENT_PATH}/models/job_ideal_model.pth\", get_bt_regressor(\"none\", DEVICE))\n",
    "\n",
    "sq_ideal_model = NN(\n",
    "    fit_settings=ALL_SS, \n",
    "    inference_settings=EMPTY_SS, \n",
    "    model=get_bt_regressor(\"sq_ideal_model\", DEVICE),\n",
    "    path_to_save=f\"{EXPERIMENT_PATH}/models/sq_ideal_model.pth\"\n",
    ")\n",
    "sq_ideal_model.fit(sq_workload, epochs=epochs)\n",
    "sq_ideal_model = load_model(DEVICE, f\"{EXPERIMENT_PATH}/models/sq_ideal_model.pth\", get_bt_regressor(\"none\", DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_df(df):\n",
    "    df[\"ex boost (% of opt)\"] = 100 * (df[\"def_ex\"] - df[\"custom_ex\"]) / (df[\"def_ex\"] - df[\"opt_ex\"])\n",
    "    df[\"e2e boost (% of opt)\"] = 100 * (df[\"def_e2e\"] - df[\"custom_e2e\"]) / (df[\"def_e2e\"] - df[\"opt_e2e\"])\n",
    "    df[\"e2e boost (%)\"] = 100 * (df[\"def_e2e\"] - df[\"custom_e2e\"]) / df[\"def_e2e\"]\n",
    "    \n",
    "    columns = [\n",
    "        \"model\", \n",
    "        \"searching_settings\", \n",
    "        \"workload\", \n",
    "        \"e2e boost (%)\", \n",
    "        \"e2e boost (% of opt)\", \n",
    "        \"ex boost (% of opt)\", \n",
    "        \"n_timeouts (%)\", \n",
    "        \"n_real_degradations (%)\",\n",
    "        \"custom_e2e\", \n",
    "        \"custom_ex\", \n",
    "        \"custom_inference\", \n",
    "        \"only_def_dop\", \n",
    "    ]\n",
    "    \n",
    "    def count_real_degradations(predictions):\n",
    "        return sum(\n",
    "            _get_e2e_time(q_n, hs, dop, False) > 1.1 * _get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP, False)\n",
    "            for q_n, hs, dop in predictions\n",
    "        )\n",
    "    sizes = df[\"predictions\"].apply(lambda el: len(el))\n",
    "    df[\"n_timeouts (%)\"] = 100 * df[\"n_timeouts\"].apply(lambda el: int(el)) / sizes\n",
    "    df[\"n_real_degradations (%)\"] = 100 * df[\"predictions\"].apply(count_real_degradations) / sizes\n",
    "    df = df[columns]\n",
    "    return df.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_and_descrs = [\n",
    "    (GREEDY_DEF_DOP_SS, \"greedy\"), \n",
    "    (PRUNED_GREEDY_DEF_DOP_SS, \"pruned greedy\"), \n",
    "    (LOCAL_DEF_DOP_SS, \"local\"),\n",
    "    (PRUNED_LOCAL_DEF_DOP_SS, \"pruned local\"),\n",
    "    (ALL_DEF_DOP_SS, \"exhaustive\")\n",
    "]\n",
    "\n",
    "def_dop_ideal_reports = []\n",
    "for ss, ss_descr in ss_and_descrs:\n",
    "    ideal_job_nn_model = NN(fit_settings=EMPTY_SS, inference_settings=ss, model=get_bt_regressor(\"ideal_job\", DEVICE))\n",
    "    ideal_job_nn_model.model = load_model(device=DEVICE, path=f\"{EXPERIMENT_PATH}/models/job_ideal_model.pth\", model=ideal_job_nn_model.model)\n",
    "    def_dop_ideal_reports.append(get_report(ideal_job_nn_model, \"NN\", job_workload, \"JOB\", ss, ss_descr, only_def_dop=True))\n",
    "\n",
    "    ideal_job_hero_model = Hero(fit_settings=ss)\n",
    "    ideal_job_hero_model.fit(job_workload)\n",
    "    def_dop_ideal_reports.append(get_report(ideal_job_hero_model, \"Hero\", job_workload, \"JOB\", ss, ss_descr, only_def_dop=True))\n",
    "    \n",
    "    ideal_sq_nn_model = NN(fit_settings=EMPTY_SS, inference_settings=ss, model=get_bt_regressor(\"ideal_sq\", DEVICE))\n",
    "    ideal_sq_nn_model.model = load_model(device=DEVICE, path=f\"{EXPERIMENT_PATH}/models/sq_ideal_model.pth\", model=ideal_sq_nn_model.model)\n",
    "    def_dop_ideal_reports.append(get_report(ideal_sq_nn_model, \"NN\", sq_workload, \"SQ\", ss, ss_descr, only_def_dop=True))\n",
    "\n",
    "    ideal_sq_hero_model = Hero(fit_settings=ss)\n",
    "    ideal_sq_hero_model.fit(sq_workload)\n",
    "    def_dop_ideal_reports.append(get_report(ideal_sq_hero_model, \"Hero\", sq_workload, \"SQ\", ss, ss_descr, only_def_dop=True))\n",
    "\n",
    "with open(f\"{ARTIFACTS_PATH}/def_dop_ideal_reports.json\", \"w\") as f:\n",
    "    dump(def_dop_ideal_reports, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ARTIFACTS_PATH}/def_dop_ideal_reports.json\", \"r\") as f:\n",
    "    def_dop_df = extend_df(pd.DataFrame(load(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>searching_settings</th>\n",
       "      <th>workload</th>\n",
       "      <th>e2e boost (%)</th>\n",
       "      <th>e2e boost (% of opt)</th>\n",
       "      <th>ex boost (% of opt)</th>\n",
       "      <th>n_timeouts (%)</th>\n",
       "      <th>n_real_degradations (%)</th>\n",
       "      <th>custom_e2e</th>\n",
       "      <th>custom_ex</th>\n",
       "      <th>custom_inference</th>\n",
       "      <th>only_def_dop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hero</td>\n",
       "      <td>local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>46.8</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291.1</td>\n",
       "      <td>266.8</td>\n",
       "      <td>24.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hero</td>\n",
       "      <td>greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>45.3</td>\n",
       "      <td>94.0</td>\n",
       "      <td>91.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>298.9</td>\n",
       "      <td>276.6</td>\n",
       "      <td>22.3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>38.4</td>\n",
       "      <td>79.6</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>336.9</td>\n",
       "      <td>281.3</td>\n",
       "      <td>55.6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>34.2</td>\n",
       "      <td>70.9</td>\n",
       "      <td>87.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>359.7</td>\n",
       "      <td>284.5</td>\n",
       "      <td>75.3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>28.5</td>\n",
       "      <td>59.2</td>\n",
       "      <td>61.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>390.6</td>\n",
       "      <td>335.4</td>\n",
       "      <td>55.3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>24.7</td>\n",
       "      <td>51.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>411.8</td>\n",
       "      <td>339.6</td>\n",
       "      <td>72.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN</td>\n",
       "      <td>local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>16.1</td>\n",
       "      <td>33.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>458.4</td>\n",
       "      <td>269.5</td>\n",
       "      <td>188.9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.1</td>\n",
       "      <td>89.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>464.6</td>\n",
       "      <td>281.2</td>\n",
       "      <td>183.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>JOB</td>\n",
       "      <td>11.7</td>\n",
       "      <td>24.2</td>\n",
       "      <td>97.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>482.8</td>\n",
       "      <td>265.0</td>\n",
       "      <td>217.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hero</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>JOB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>546.7</td>\n",
       "      <td>455.3</td>\n",
       "      <td>91.4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model searching_settings workload  e2e boost (%)  e2e boost (% of opt)  \\\n",
       "9   Hero              local      JOB           46.8                  97.0   \n",
       "1   Hero             greedy      JOB           45.3                  94.0   \n",
       "13  Hero       pruned local      JOB           38.4                  79.6   \n",
       "12    NN       pruned local      JOB           34.2                  70.9   \n",
       "5   Hero      pruned greedy      JOB           28.5                  59.2   \n",
       "4     NN      pruned greedy      JOB           24.7                  51.2   \n",
       "8     NN              local      JOB           16.1                  33.5   \n",
       "0     NN             greedy      JOB           15.0                  31.1   \n",
       "16    NN         exhaustive      JOB           11.7                  24.2   \n",
       "17  Hero         exhaustive      JOB            0.0                   0.0   \n",
       "\n",
       "    ex boost (% of opt)  n_timeouts (%)  n_real_degradations (%)  custom_e2e  \\\n",
       "9                  96.4             0.0                      0.0       291.1   \n",
       "1                  91.4             0.0                      0.0       298.9   \n",
       "13                 89.0             0.0                      0.0       336.9   \n",
       "12                 87.4             0.0                      1.8       359.7   \n",
       "5                  61.3             0.0                      0.0       390.6   \n",
       "4                  59.2             0.0                      1.8       411.8   \n",
       "8                  95.0             0.9                      1.8       458.4   \n",
       "0                  89.1             0.0                      0.9       464.6   \n",
       "16                 97.3             5.3                      7.1       482.8   \n",
       "17                  0.0             0.0                      0.0       546.7   \n",
       "\n",
       "    custom_ex  custom_inference  only_def_dop  \n",
       "9       266.8              24.2          True  \n",
       "1       276.6              22.3          True  \n",
       "13      281.3              55.6          True  \n",
       "12      284.5              75.3          True  \n",
       "5       335.4              55.3          True  \n",
       "4       339.6              72.2          True  \n",
       "8       269.5             188.9          True  \n",
       "0       281.2             183.5          True  \n",
       "16      265.0             217.8          True  \n",
       "17      455.3              91.4          True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_dop_df[(def_dop_df[\"workload\"] == \"JOB\")].sort_values(by=\"e2e boost (% of opt)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>searching_settings</th>\n",
       "      <th>workload</th>\n",
       "      <th>e2e boost (%)</th>\n",
       "      <th>e2e boost (% of opt)</th>\n",
       "      <th>ex boost (% of opt)</th>\n",
       "      <th>n_timeouts (%)</th>\n",
       "      <th>n_real_degradations (%)</th>\n",
       "      <th>custom_e2e</th>\n",
       "      <th>custom_ex</th>\n",
       "      <th>custom_inference</th>\n",
       "      <th>only_def_dop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hero</td>\n",
       "      <td>local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>64.5</td>\n",
       "      <td>93.9</td>\n",
       "      <td>96.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.2</td>\n",
       "      <td>240.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hero</td>\n",
       "      <td>greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>63.2</td>\n",
       "      <td>92.0</td>\n",
       "      <td>93.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.6</td>\n",
       "      <td>254.2</td>\n",
       "      <td>29.4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>58.3</td>\n",
       "      <td>84.9</td>\n",
       "      <td>89.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>320.7</td>\n",
       "      <td>273.4</td>\n",
       "      <td>47.3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>54.0</td>\n",
       "      <td>78.7</td>\n",
       "      <td>89.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>353.9</td>\n",
       "      <td>273.7</td>\n",
       "      <td>80.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>53.0</td>\n",
       "      <td>77.1</td>\n",
       "      <td>80.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.9</td>\n",
       "      <td>318.3</td>\n",
       "      <td>43.6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NN</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>SQ</td>\n",
       "      <td>52.6</td>\n",
       "      <td>76.5</td>\n",
       "      <td>99.4</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>365.1</td>\n",
       "      <td>223.8</td>\n",
       "      <td>141.3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>50.5</td>\n",
       "      <td>73.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>380.8</td>\n",
       "      <td>318.9</td>\n",
       "      <td>61.9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN</td>\n",
       "      <td>greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>44.9</td>\n",
       "      <td>65.4</td>\n",
       "      <td>92.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>424.1</td>\n",
       "      <td>259.5</td>\n",
       "      <td>164.6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN</td>\n",
       "      <td>local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>40.8</td>\n",
       "      <td>59.4</td>\n",
       "      <td>95.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>455.5</td>\n",
       "      <td>243.6</td>\n",
       "      <td>211.9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hero</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>SQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>769.7</td>\n",
       "      <td>711.5</td>\n",
       "      <td>58.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model searching_settings workload  e2e boost (%)  e2e boost (% of opt)  \\\n",
       "11  Hero              local       SQ           64.5                  93.9   \n",
       "3   Hero             greedy       SQ           63.2                  92.0   \n",
       "15  Hero       pruned local       SQ           58.3                  84.9   \n",
       "14    NN       pruned local       SQ           54.0                  78.7   \n",
       "7   Hero      pruned greedy       SQ           53.0                  77.1   \n",
       "18    NN         exhaustive       SQ           52.6                  76.5   \n",
       "6     NN      pruned greedy       SQ           50.5                  73.6   \n",
       "2     NN             greedy       SQ           44.9                  65.4   \n",
       "10    NN              local       SQ           40.8                  59.4   \n",
       "19  Hero         exhaustive       SQ            0.0                   0.0   \n",
       "\n",
       "    ex boost (% of opt)  n_timeouts (%)  n_real_degradations (%)  custom_e2e  \\\n",
       "11                 96.1             0.0                      0.0       273.2   \n",
       "3                  93.2             0.0                      0.0       283.6   \n",
       "15                 89.3             0.0                      0.0       320.7   \n",
       "14                 89.2             0.0                      2.5       353.9   \n",
       "7                  80.1             0.0                      0.0       361.9   \n",
       "18                 99.4             7.5                     10.0       365.1   \n",
       "6                  80.0             0.0                      2.5       380.8   \n",
       "2                  92.1             0.0                      2.5       424.1   \n",
       "10                 95.3             5.0                      7.5       455.5   \n",
       "19                  0.0             0.0                      0.0       769.7   \n",
       "\n",
       "    custom_ex  custom_inference  only_def_dop  \n",
       "11      240.1              33.1          True  \n",
       "3       254.2              29.4          True  \n",
       "15      273.4              47.3          True  \n",
       "14      273.7              80.2          True  \n",
       "7       318.3              43.6          True  \n",
       "18      223.8             141.3          True  \n",
       "6       318.9              61.9          True  \n",
       "2       259.5             164.6          True  \n",
       "10      243.6             211.9          True  \n",
       "19      711.5              58.2          True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_dop_df[(def_dop_df[\"workload\"] == \"SQ\")].sort_values(by=\"e2e boost (% of opt)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Dop's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_and_descrs = [\n",
    "    (GREEDY_SS, \"greedy\"), \n",
    "    (PRUNED_GREEDY_SS, \"pruned greedy\"), \n",
    "    (LOCAL_SS, \"local\"),\n",
    "    (PRUNED_LOCAL_SS, \"pruned local\"),\n",
    "    (ALL_SS, \"exhaustive\")\n",
    "]\n",
    "\n",
    "all_dops_ideal_reports = []\n",
    "for ss, ss_descr in ss_and_descrs:\n",
    "    ideal_job_nn_model = NN(fit_settings=EMPTY_SS, inference_settings=ss, model=get_bt_regressor(\"ideal_job\", DEVICE))\n",
    "    ideal_job_nn_model.model = load_model(device=DEVICE, path=f\"{EXPERIMENT_PATH}/models/job_ideal_model.pth\", model=ideal_job_nn_model.model)\n",
    "    all_dops_ideal_reports.append(get_report(ideal_job_nn_model, \"NN\", job_workload, \"JOB\", ss, ss_descr, only_def_dop=False))\n",
    "\n",
    "    ideal_job_hero_model = Hero(fit_settings=ss)\n",
    "    ideal_job_hero_model.fit(job_workload)\n",
    "    all_dops_ideal_reports.append(get_report(ideal_job_hero_model, \"Hero\", job_workload, \"JOB\", ss, ss_descr, only_def_dop=False))\n",
    "    \n",
    "    ideal_sq_nn_model = NN(fit_settings=EMPTY_SS, inference_settings=ss, model=get_bt_regressor(\"ideal_sq\", DEVICE))\n",
    "    ideal_sq_nn_model.model = load_model(device=DEVICE, path=f\"{EXPERIMENT_PATH}/models/sq_ideal_model.pth\", model=ideal_sq_nn_model.model)\n",
    "    all_dops_ideal_reports.append(get_report(ideal_sq_nn_model, \"NN\", sq_workload, \"SQ\", ss, ss_descr, only_def_dop=False))\n",
    "\n",
    "    ideal_sq_hero_model = Hero(fit_settings=ss)\n",
    "    ideal_sq_hero_model.fit(sq_workload)\n",
    "    all_dops_ideal_reports.append(get_report(ideal_sq_hero_model, \"Hero\", sq_workload, \"SQ\", ss, ss_descr, only_def_dop=False))\n",
    "\n",
    "with open(f\"{ARTIFACTS_PATH}/all_dops_ideal_reports.json\", \"w\") as f:\n",
    "    dump(all_dops_ideal_reports, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ARTIFACTS_PATH}/all_dops_ideal_reports.json\", \"r\") as f:\n",
    "    all_dops_df = extend_df(pd.DataFrame(load(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>searching_settings</th>\n",
       "      <th>workload</th>\n",
       "      <th>e2e boost (%)</th>\n",
       "      <th>e2e boost (% of opt)</th>\n",
       "      <th>ex boost (% of opt)</th>\n",
       "      <th>n_timeouts (%)</th>\n",
       "      <th>n_real_degradations (%)</th>\n",
       "      <th>custom_e2e</th>\n",
       "      <th>custom_ex</th>\n",
       "      <th>custom_inference</th>\n",
       "      <th>only_def_dop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hero</td>\n",
       "      <td>local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>64.6</td>\n",
       "      <td>98.4</td>\n",
       "      <td>98.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.4</td>\n",
       "      <td>166.7</td>\n",
       "      <td>26.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>61.2</td>\n",
       "      <td>93.3</td>\n",
       "      <td>97.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>171.9</td>\n",
       "      <td>40.1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>56.2</td>\n",
       "      <td>85.7</td>\n",
       "      <td>94.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>239.2</td>\n",
       "      <td>180.7</td>\n",
       "      <td>58.5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hero</td>\n",
       "      <td>greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>51.2</td>\n",
       "      <td>78.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.7</td>\n",
       "      <td>249.5</td>\n",
       "      <td>17.2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN</td>\n",
       "      <td>local</td>\n",
       "      <td>JOB</td>\n",
       "      <td>37.5</td>\n",
       "      <td>57.2</td>\n",
       "      <td>97.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>341.4</td>\n",
       "      <td>170.7</td>\n",
       "      <td>170.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>33.2</td>\n",
       "      <td>50.5</td>\n",
       "      <td>49.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>365.4</td>\n",
       "      <td>312.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>28.7</td>\n",
       "      <td>43.7</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>389.8</td>\n",
       "      <td>318.3</td>\n",
       "      <td>71.5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>greedy</td>\n",
       "      <td>JOB</td>\n",
       "      <td>20.2</td>\n",
       "      <td>30.8</td>\n",
       "      <td>68.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>436.1</td>\n",
       "      <td>255.7</td>\n",
       "      <td>180.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hero</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>JOB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>546.7</td>\n",
       "      <td>455.3</td>\n",
       "      <td>91.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>JOB</td>\n",
       "      <td>-17.8</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>98.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>10.6</td>\n",
       "      <td>643.7</td>\n",
       "      <td>168.6</td>\n",
       "      <td>475.2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model searching_settings workload  e2e boost (%)  e2e boost (% of opt)  \\\n",
       "9   Hero              local      JOB           64.6                  98.4   \n",
       "13  Hero       pruned local      JOB           61.2                  93.3   \n",
       "12    NN       pruned local      JOB           56.2                  85.7   \n",
       "1   Hero             greedy      JOB           51.2                  78.0   \n",
       "8     NN              local      JOB           37.5                  57.2   \n",
       "5   Hero      pruned greedy      JOB           33.2                  50.5   \n",
       "4     NN      pruned greedy      JOB           28.7                  43.7   \n",
       "0     NN             greedy      JOB           20.2                  30.8   \n",
       "17  Hero         exhaustive      JOB            0.0                   0.0   \n",
       "16    NN         exhaustive      JOB          -17.8                 -27.0   \n",
       "\n",
       "    ex boost (% of opt)  n_timeouts (%)  n_real_degradations (%)  custom_e2e  \\\n",
       "9                  98.9             0.0                      0.0       193.4   \n",
       "13                 97.2             0.0                      0.0       212.0   \n",
       "12                 94.1             0.9                      2.7       239.2   \n",
       "1                  70.5             0.0                      0.0       266.7   \n",
       "8                  97.5             4.4                      5.3       341.4   \n",
       "5                  49.1             0.0                      0.9       365.4   \n",
       "4                  47.0             0.0                      1.8       389.8   \n",
       "0                  68.4             1.8                      2.7       436.1   \n",
       "17                  0.0             0.0                      0.0       546.7   \n",
       "16                 98.3             8.8                     10.6       643.7   \n",
       "\n",
       "    custom_ex  custom_inference  only_def_dop  \n",
       "9       166.7              26.7         False  \n",
       "13      171.9              40.1         False  \n",
       "12      180.7              58.5         False  \n",
       "1       249.5              17.2         False  \n",
       "8       170.7             170.7         False  \n",
       "5       312.0              53.5         False  \n",
       "4       318.3              71.5         False  \n",
       "0       255.7             180.4         False  \n",
       "17      455.3              91.4         False  \n",
       "16      168.6             475.2         False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dops_df[(all_dops_df[\"workload\"] == \"JOB\")].sort_values(by=\"e2e boost (% of opt)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>searching_settings</th>\n",
       "      <th>workload</th>\n",
       "      <th>e2e boost (%)</th>\n",
       "      <th>e2e boost (% of opt)</th>\n",
       "      <th>ex boost (% of opt)</th>\n",
       "      <th>n_timeouts (%)</th>\n",
       "      <th>n_real_degradations (%)</th>\n",
       "      <th>custom_e2e</th>\n",
       "      <th>custom_ex</th>\n",
       "      <th>custom_inference</th>\n",
       "      <th>only_def_dop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hero</td>\n",
       "      <td>local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>69.6</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>208.4</td>\n",
       "      <td>25.6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>66.9</td>\n",
       "      <td>90.4</td>\n",
       "      <td>91.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.6</td>\n",
       "      <td>224.2</td>\n",
       "      <td>30.3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hero</td>\n",
       "      <td>greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>65.3</td>\n",
       "      <td>88.2</td>\n",
       "      <td>89.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.4</td>\n",
       "      <td>235.3</td>\n",
       "      <td>32.1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>59.1</td>\n",
       "      <td>79.8</td>\n",
       "      <td>90.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>314.9</td>\n",
       "      <td>230.1</td>\n",
       "      <td>84.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hero</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>54.9</td>\n",
       "      <td>74.2</td>\n",
       "      <td>76.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>347.1</td>\n",
       "      <td>305.1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN</td>\n",
       "      <td>pruned greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>52.3</td>\n",
       "      <td>70.7</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>366.9</td>\n",
       "      <td>304.6</td>\n",
       "      <td>62.3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN</td>\n",
       "      <td>local</td>\n",
       "      <td>SQ</td>\n",
       "      <td>47.7</td>\n",
       "      <td>64.5</td>\n",
       "      <td>94.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>402.3</td>\n",
       "      <td>212.0</td>\n",
       "      <td>190.3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN</td>\n",
       "      <td>greedy</td>\n",
       "      <td>SQ</td>\n",
       "      <td>44.3</td>\n",
       "      <td>59.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>428.6</td>\n",
       "      <td>244.2</td>\n",
       "      <td>184.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NN</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>SQ</td>\n",
       "      <td>38.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>98.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>477.1</td>\n",
       "      <td>189.4</td>\n",
       "      <td>287.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hero</td>\n",
       "      <td>exhaustive</td>\n",
       "      <td>SQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>769.7</td>\n",
       "      <td>711.5</td>\n",
       "      <td>58.2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model searching_settings workload  e2e boost (%)  e2e boost (% of opt)  \\\n",
       "11  Hero              local       SQ           69.6                  94.0   \n",
       "15  Hero       pruned local       SQ           66.9                  90.4   \n",
       "3   Hero             greedy       SQ           65.3                  88.2   \n",
       "14    NN       pruned local       SQ           59.1                  79.8   \n",
       "7   Hero      pruned greedy       SQ           54.9                  74.2   \n",
       "6     NN      pruned greedy       SQ           52.3                  70.7   \n",
       "10    NN              local       SQ           47.7                  64.5   \n",
       "2     NN             greedy       SQ           44.3                  59.9   \n",
       "18    NN         exhaustive       SQ           38.0                  51.4   \n",
       "19  Hero         exhaustive       SQ            0.0                   0.0   \n",
       "\n",
       "    ex boost (% of opt)  n_timeouts (%)  n_real_degradations (%)  custom_e2e  \\\n",
       "11                 94.8             0.0                      0.0       234.0   \n",
       "15                 91.8             0.0                      0.0       254.6   \n",
       "3                  89.7             0.0                      0.0       267.4   \n",
       "14                 90.7             0.0                      2.5       314.9   \n",
       "7                  76.6             0.0                      0.0       347.1   \n",
       "6                  76.7             0.0                      2.5       366.9   \n",
       "10                 94.1             5.0                      7.5       402.3   \n",
       "2                  88.0             5.0                      7.5       428.6   \n",
       "18                 98.4             5.0                      7.5       477.1   \n",
       "19                  0.0             0.0                      0.0       769.7   \n",
       "\n",
       "    custom_ex  custom_inference  only_def_dop  \n",
       "11      208.4              25.6         False  \n",
       "15      224.2              30.3         False  \n",
       "3       235.3              32.1         False  \n",
       "14      230.1              84.7         False  \n",
       "7       305.1              42.0         False  \n",
       "6       304.6              62.3         False  \n",
       "10      212.0             190.3         False  \n",
       "2       244.2             184.4         False  \n",
       "18      189.4             287.7         False  \n",
       "19      711.5              58.2         False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dops_df[(all_dops_df[\"workload\"] == \"SQ\")].sort_values(by=\"e2e boost (% of opt)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even in the ideal scenario, the NN loses a bit, mainly due to the longer inference time. Moreover, it sometimes leads to degradations and even `T/O` (this probably happens on small queries).\n",
    "\n",
    "The advantage of a `PRUNED LOCAL` strategy is also evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Scenario\n",
    "\n",
    "Collecting train data during online optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20/100] MSE: 73.0167:  20%|██        | 20/100 [01:01<04:07,  3.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m epochs, iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ss, ss_descr \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m      7\u001b[0m     (GREEDY_DEF_DOP_SS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGREEDY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#(PRUNED_GREEDY_DEF_DOP_SS, \"PRUNED GREEDY\"),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#(ALL_DEF_DOP_SS, \"EXHAUSTIVE\"),\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ]:\n\u001b[0;32m---> 13\u001b[0m     def_dop_list_online_reports\u001b[38;5;241m.\u001b[39mappend(\u001b[43memulate_online_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkload_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss_descr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ss, ss_descr \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     16\u001b[0m     (GREEDY_SS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGREEDY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#(PRUNED_GREEDY_SS, \"PRUNED GREEDY\"),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#(ALL_SS, \"EXHAUSTIVE\"),\u001b[39;00m\n\u001b[1;32m     21\u001b[0m ]:          \n\u001b[1;32m     22\u001b[0m     all_dops_list_online_reports\u001b[38;5;241m.\u001b[39mappend(emulate_online_learning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, workload, workload_name, ss, ss_descr, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, epochs, iterations, \u001b[38;5;28;01mNone\u001b[39;00m, DEVICE))\n",
      "File \u001b[0;32m/notebooks/hero/emulation.py:116\u001b[0m, in \u001b[0;36memulate_online_learning\u001b[0;34m(model_name, workload, workload_name, ss, ss_descr, only_def_dop, with_default_data, epochs, n_runs, path_to_save, device)\u001b[0m\n\u001b[1;32m    114\u001b[0m     default_history \u001b[38;5;241m=\u001b[39m [(q_n, DEFAULT_HINTSET, DEFAULT_DOP) \u001b[38;5;28;01mfor\u001b[39;00m q_n \u001b[38;5;129;01min\u001b[39;00m workload]\n\u001b[1;32m    115\u001b[0m     model\u001b[38;5;241m.\u001b[39madd_history(default_history)\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m reports\u001b[38;5;241m.\u001b[39mappend(get_report(model, model_name, workload, workload_name, ss, ss_descr, only_def_dop))\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_runs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/notebooks/hero/neural_network.py:232\u001b[0m, in \u001b[0;36mNN.update_model\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    231\u001b[0m set_seed(\u001b[38;5;241m2024\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m \u001b[43mweighted_train_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/hero/train_utils.py:86\u001b[0m, in \u001b[0;36mweighted_train_loop\u001b[0;34m(model, optimizer, criterion, scheduler, train_dataloader, num_epochs, start_epoch, ckpt_period, path_to_save)\u001b[0m\n\u001b[1;32m     84\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, start_epoch \u001b[38;5;241m+\u001b[39m num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39mtqdm_desc, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m---> 86\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[1;32m     88\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/notebooks/hero/train_utils.py:64\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(model, optimizer, criterion, dataloader, train_mode)\u001b[0m\n\u001b[1;32m     61\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (freq\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), time))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mweighted_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     67\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weighted_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m vertices\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for workload, workload_name in [(job_workload, \"JOB\"), (sq_workload, \"SQ\")]:\n",
    "    def_dop_list_online_reports = []\n",
    "    all_dops_list_online_reports = []\n",
    "\n",
    "    epochs, iterations = 100, 5\n",
    "    for ss, ss_descr in [\n",
    "        (GREEDY_DEF_DOP_SS, \"GREEDY\"),\n",
    "        #(PRUNED_GREEDY_DEF_DOP_SS, \"PRUNED GREEDY\"),\n",
    "        (LOCAL_DEF_DOP_SS, \"LOCAL\"),\n",
    "        (PRUNED_LOCAL_DEF_DOP_SS, \"PRUNED LOCAL\"),\n",
    "        #(ALL_DEF_DOP_SS, \"EXHAUSTIVE\"),\n",
    "    ]:\n",
    "        def_dop_list_online_reports.append(emulate_online_learning(\"NN\", workload, workload_name, ss, ss_descr, True, True, epochs, iterations, None, DEVICE))\n",
    "\n",
    "    for ss, ss_descr in [\n",
    "        (GREEDY_SS, \"GREEDY\"),\n",
    "        #(PRUNED_GREEDY_SS, \"PRUNED GREEDY\"),\n",
    "        (LOCAL_SS, \"LOCAL\"),\n",
    "        (PRUNED_LOCAL_SS, \"PRUNED LOCAL\"),\n",
    "        #(ALL_SS, \"EXHAUSTIVE\"),\n",
    "    ]:          \n",
    "        all_dops_list_online_reports.append(emulate_online_learning(\"NN\", workload, workload_name, ss, ss_descr, False, True, epochs, iterations, None, DEVICE))\n",
    "    \n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_def_dop_list_online_reports.json\", \"w\") as f:\n",
    "        dump(def_dop_list_online_reports, f)\n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_all_dops_list_online_reports.json\", \"w\") as f:\n",
    "        dump(all_dops_list_online_reports, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100/100] MSE: 0.1894: 100%|██████████| 100/100 [01:04<00:00,  1.54it/s]\n",
      "[100/100] MSE: 0.5487: 100%|██████████| 100/100 [06:46<00:00,  4.06s/it]\n",
      "[17/100] MSE: 12.4115:  17%|█▋        | 17/100 [00:17<01:23,  1.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m epochs, iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ss, ss_descr \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m      7\u001b[0m     (GREEDY_DEF_DOP_SS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGREEDY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#(PRUNED_GREEDY_DEF_DOP_SS, \"PRUNED GREEDY\"),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#(ALL_DEF_DOP_SS, \"EXHAUSTIVE\"),\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ]:\n\u001b[0;32m---> 13\u001b[0m     def_dop_list_online_reports\u001b[38;5;241m.\u001b[39mappend(\u001b[43memulate_online_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkload_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss_descr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ss, ss_descr \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     16\u001b[0m     (GREEDY_SS, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGREEDY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#(PRUNED_GREEDY_SS, \"PRUNED GREEDY\"),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#(ALL_SS, \"EXHAUSTIVE\"),\u001b[39;00m\n\u001b[1;32m     21\u001b[0m ]:          \n\u001b[1;32m     22\u001b[0m     all_dops_list_online_reports\u001b[38;5;241m.\u001b[39mappend(emulate_online_learning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, workload, workload_name, ss, ss_descr, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, epochs, iterations, \u001b[38;5;28;01mNone\u001b[39;00m, DEVICE))\n",
      "File \u001b[0;32m/notebooks/hero/emulation.py:122\u001b[0m, in \u001b[0;36memulate_online_learning\u001b[0;34m(model_name, workload, workload_name, ss, ss_descr, only_def_dop, with_default_data, epochs, n_runs, path_to_save, device)\u001b[0m\n\u001b[1;32m    120\u001b[0m     collected_history \u001b[38;5;241m=\u001b[39m reports[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    121\u001b[0m     model\u001b[38;5;241m.\u001b[39madd_history(collected_history)\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     reports\u001b[38;5;241m.\u001b[39mappend(get_report(model, model_name, workload, workload_name, ss, ss_descr, only_def_dop))\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reports\n",
      "File \u001b[0;32m/notebooks/hero/neural_network.py:232\u001b[0m, in \u001b[0;36mNN.update_model\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    231\u001b[0m set_seed(\u001b[38;5;241m2024\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m \u001b[43mweighted_train_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/hero/train_utils.py:86\u001b[0m, in \u001b[0;36mweighted_train_loop\u001b[0;34m(model, optimizer, criterion, scheduler, train_dataloader, num_epochs, start_epoch, ckpt_period, path_to_save)\u001b[0m\n\u001b[1;32m     84\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, start_epoch \u001b[38;5;241m+\u001b[39m num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39mtqdm_desc, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m---> 86\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[1;32m     88\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/notebooks/hero/train_utils.py:60\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(model, optimizer, criterion, dataloader, train_mode)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[1;32m     58\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (freq\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), time))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/hero/btcnn/regressor.py:23\u001b[0m, in \u001b[0;36mBinaryTreeRegressor.forward\u001b[0;34m(self, vertices, edges)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, vertices: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, edges: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcnn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbtcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/hero/btcnn/layers.py:107\u001b[0m, in \u001b[0;36mBinaryTreeSequential.forward\u001b[0;34m(self, vertices, edges)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, vertices: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, edges: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 107\u001b[0m         vertices \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vertices\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/hero/btcnn/layers.py:82\u001b[0m, in \u001b[0;36mBinaryTreeConv.forward\u001b[0;34m(self, vertices, edges)\u001b[0m\n\u001b[1;32m     80\u001b[0m _check_shapes(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, vertices\u001b[38;5;241m=\u001b[39mvertices, edges\u001b[38;5;241m=\u001b[39medges)\n\u001b[1;32m     81\u001b[0m padded_vertices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_padding_vertex(vertices\u001b[38;5;241m=\u001b[39mvertices)\n\u001b[0;32m---> 82\u001b[0m flattened_neighborhoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flatten_neighborhoods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_vertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m convoluted_vertices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d(flattened_neighborhoods)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convoluted_vertices\n",
      "File \u001b[0;32m/notebooks/hero/btcnn/layers.py:89\u001b[0m, in \u001b[0;36mBinaryTreeConv._flatten_neighborhoods\u001b[0;34m(self, vertices, edges)\u001b[0m\n\u001b[1;32m     87\u001b[0m all_channels_edges \u001b[38;5;241m=\u001b[39m edges\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m neighborhood_backbone \u001b[38;5;241m=\u001b[39m vertices\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, NEIGHBORHOOD_SIZE)\n\u001b[0;32m---> 89\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighborhood_backbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVERTICES_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_channels_edges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m flattened_neighborhoods \u001b[38;5;241m=\u001b[39m neighborhoods\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flattened_neighborhoods\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for workload, workload_name in [(job_workload, \"JOB\"), (sq_workload, \"SQ\")]:\n",
    "    def_dop_list_online_reports = []\n",
    "    all_dops_list_online_reports = []\n",
    "\n",
    "    epochs, iterations = 100, 5\n",
    "    for ss, ss_descr in [\n",
    "        (GREEDY_DEF_DOP_SS, \"GREEDY\"),\n",
    "        #(PRUNED_GREEDY_DEF_DOP_SS, \"PRUNED GREEDY\"),\n",
    "        (LOCAL_DEF_DOP_SS, \"LOCAL\"),\n",
    "        (PRUNED_LOCAL_DEF_DOP_SS, \"PRUNED LOCAL\"),\n",
    "        #(ALL_DEF_DOP_SS, \"EXHAUSTIVE\"),\n",
    "    ]:\n",
    "        def_dop_list_online_reports.append(emulate_online_learning(\"NN\", workload, workload_name, ss, ss_descr, True, True, epochs, iterations, None, DEVICE))\n",
    "\n",
    "    for ss, ss_descr in [\n",
    "        (GREEDY_SS, \"GREEDY\"),\n",
    "        #(PRUNED_GREEDY_SS, \"PRUNED GREEDY\"),\n",
    "        (LOCAL_SS, \"LOCAL\"),\n",
    "        (PRUNED_LOCAL_SS, \"PRUNED LOCAL\"),\n",
    "        #(ALL_SS, \"EXHAUSTIVE\"),\n",
    "    ]:          \n",
    "        all_dops_list_online_reports.append(emulate_online_learning(\"NN\", workload, workload_name, ss, ss_descr, False, True, epochs, iterations, None, DEVICE))\n",
    "    \n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_def_dop_list_online_reports.json\", \"w\") as f:\n",
    "        dump(def_dop_list_online_reports, f)\n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_all_dops_list_online_reports.json\", \"w\") as f:\n",
    "        dump(all_dops_list_online_reports, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(list_reports, title):\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    colors = {\n",
    "        \"opt\": \"green\",\n",
    "        \"def\": \"orange\"\n",
    "    }\n",
    "\n",
    "    x_values = np.arange(len(list_reports[0]))\n",
    "\n",
    "    small_line, big_line = 2, 3\n",
    "    for i, report in enumerate(list_reports):\n",
    "        color = plt.cm.viridis(i / len(list_reports))\n",
    "        ax.plot(\n",
    "            x_values, \n",
    "            [el[\"custom_ex\"] for el in report], \n",
    "            marker=\"o\", \n",
    "            linewidth=small_line, \n",
    "            linestyle='--', \n",
    "            color=color, \n",
    "            label=f'{report[0][\"searching_settings\"]} Ex'\n",
    "        )\n",
    "        ax.plot(\n",
    "            x_values, \n",
    "            [el[\"custom_e2e\"] for el in report], \n",
    "            marker=\"o\", \n",
    "            linewidth=big_line, \n",
    "            markersize=10.0, \n",
    "            linestyle='-', \n",
    "            color=color, \n",
    "            label=f'{report[0][\"searching_settings\"]} E2E'\n",
    "        )\n",
    "\n",
    "    for metric in [\"opt\", \"def\"]:\n",
    "        for key, linewidth, linestyle, alpha in zip([\"ex\", \"e2e\"], [small_line, big_line], [\"--\", \"-\"], [0.4, 1.0]):\n",
    "            value = list_reports[0][0][f\"{metric}_{key}\"]\n",
    "            plt.plot(\n",
    "                [0, len(list_reports[0])-1], \n",
    "                [value, value], \n",
    "                linewidth=linewidth, \n",
    "                color=colors[metric], \n",
    "                alpha=alpha, \n",
    "                linestyle=linestyle, \n",
    "                label=f'{metric}_{key}'\n",
    "            )\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "    ax.set_xticks(range(len(list_reports[0])))\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"time (sec)\")\n",
    "    plt.title(f\"Convergence of online learning on benchmark {title}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for workload, workload_name in [(job_workload, \"JOB\"), (sq_workload, \"SQ\")]:\n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_def_dop_list_online_reports.json\", \"r\") as f:\n",
    "        def_dop_list_online_reports = load(f)\n",
    "        visualise(def_dop_list_online_reports, title=workload_name + \" [default DOP]\")\n",
    "\n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_all_dops_list_online_reports.json\", \"r\") as f:\n",
    "        all_dops_list_online_reports = load(f)\n",
    "        visualise(all_dops_list_online_reports, title=workload_name + \" [all DOPs]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, as the search space expands, the exhaustive algorithms stop working (at least on JOB). The superiority of the local search algorithm and the pruning procedure is also evident.\n",
    "\n",
    "Moreover, it did not always converge to the optimum even in 25 iterations - this tells us that it makes sense to take the learning procedure offline (as it is done in `Hero`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_reports, fast_reports = [], []\n",
    "\n",
    "epochs = 1\n",
    "for workload, workload_name in [(job_workload, \"JOB\"), (sq_workload, \"SQ\")]:\n",
    "    workload = [q_n for q_n in workload if _get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP) > 1]\n",
    "    print(len(workload))\n",
    "    slow_time_treshold = np.quantile([_get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP) for q_n in workload], .5)\n",
    "    slow_train = [q_n for q_n in workload if _get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP) > slow_time_treshold]\n",
    "    slow_test = [q_n for q_n in workload if _get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP) <= slow_time_treshold]\n",
    "    assert len(slow_train) + len(slow_test) == len(workload)\n",
    "    slow_nnmodel = NN(fit_settings=ALL_SS, inference_settings=EMPTY_SS, model=get_bt_regressor(\"dummy\", DEVICE))\n",
    "    slow_nnmodel.fit(slow_train, epochs)\n",
    "\n",
    "    fast_time_treshold = np.quantile([_get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP) for q_n in workload], .5)\n",
    "    fast_train = [q_n for q_n in workload if _get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP) < fast_time_treshold]\n",
    "    fast_test = [q_n for q_n in workload if _get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP) >= fast_time_treshold]\n",
    "    assert len(fast_train) + len(fast_test) == len(workload)\n",
    "    fast_nnmodel = NN(fit_settings=ALL_SS, inference_settings=EMPTY_SS, model=get_bt_regressor(\"dummy\", DEVICE))\n",
    "    fast_nnmodel.fit(fast_train, epochs)\n",
    "\n",
    "    for ss, ss_descr in [\n",
    "        (GREEDY_SS, \"GREEDY\"),\n",
    "        (PRUNED_GREEDY_SS, \"PRUNED GREEDY\"),\n",
    "        (LOCAL_SS, \"LOCAL\"),\n",
    "        (PRUNED_LOCAL_SS, \"PRUNED LOCAL\"),\n",
    "        (ALL_SS, \"EXHAUSTIVE\"),\n",
    "    ]:\n",
    "        slow_reports.append(get_report(slow_nnmodel, \"NN\", slow_train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "        slow_reports.append(get_report(slow_nnmodel, \"NN\", slow_test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "        slow_heromodel = Hero(ss)\n",
    "        slow_heromodel.fit(slow_train)\n",
    "        slow_reports.append(get_report(slow_heromodel, \"Hero\", slow_train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "        slow_reports.append(get_report(slow_heromodel, \"Hero\", slow_test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "\n",
    "        fast_reports.append(get_report(fast_nnmodel, \"NN\", fast_train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "        fast_reports.append(get_report(fast_nnmodel, \"NN\", fast_test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "        fast_heromodel = Hero(ss)\n",
    "        fast_heromodel.fit(fast_train)\n",
    "        fast_reports.append(get_report(fast_heromodel, \"Hero\", fast_train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "        fast_reports.append(get_report(fast_heromodel, \"Hero\", fast_test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "\n",
    "        with open(f\"{ARTIFACTS_PATH}/{workload_name}_slow_reports.json\", \"w\") as f:\n",
    "            dump(slow_reports, f)\n",
    "        with open(f\"{ARTIFACTS_PATH}/{workload_name}_fast_reports.json\", \"w\") as f:\n",
    "            dump(fast_reports, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ARTIFACTS_PATH}/JOB_slow_reports.json\", \"r\") as f:\n",
    "    job_slow_df = extend_df(pd.DataFrame(load(f)))\n",
    "with open(f\"{ARTIFACTS_PATH}/JOB_fast_reports.json\", \"r\") as f:\n",
    "    job_fast_df = extend_df(pd.DataFrame(load(f)))\n",
    "with open(f\"{ARTIFACTS_PATH}/SQ_slow_reports.json\", \"r\") as f:\n",
    "    sq_slow_df = extend_df(pd.DataFrame(load(f)))\n",
    "with open(f\"{ARTIFACTS_PATH}/SQ_fast_reports.json\", \"r\") as f:\n",
    "    sq_fast_df = extend_df(pd.DataFrame(load(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slot $\\rightarrow$ fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_slow_df[(job_slow_df[\"workload\"] == \"JOB[train]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_slow_df[(job_slow_df[\"workload\"] == \"JOB[test]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_slow_df[(sq_slow_df[\"workload\"] == \"SQ[train]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_slow_df[(sq_slow_df[\"workload\"] == \"SQ[test]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that `Hero` is always better on `train`, is safer on `test` (`SQ`), but sometimes it misses possible boost (`JOB`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast $\\rightarrow$ slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_fast_df[(job_fast_df[\"workload\"] == \"JOB[train]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_fast_df[(job_fast_df[\"workload\"] == \"JOB[test]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_fast_df[(sq_fast_df[\"workload\"] == \"SQ[train]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_fast_df[(sq_fast_df[\"workload\"] == \"SQ[test]\")].sort_values(by=\"e2e boost (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.**\\\n",
    "We can perfectly see the signs of **overfitting** in `Hero` (which is what we wanted) - almost perfect performance on training and safe, rare predictions on the test. On new data, we stop predicting, so we don't get degradation.\n",
    "\n",
    "The NN approach, on the other hand, has the advantage of **being able to generalise knowledge** to new queries. Thus, we can see that generalisation from fast queries on `SQ` is quite effective - we can speed up their execution by 2 times (even taking into account that we get degradations in 17% of cases). However, prediction on new queries can also bring regression, which is observed from generalising over slow queries.\n",
    "\n",
    "P.S. It is probably easier to generalise knowledge from short queries to long queries because parts of their efficient long query plans are quite fast to execute and must have already been encountered in fast query plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traintest_split(groups, ratio, seed=42, debug=False):\n",
    "    train, test = [], []\n",
    "    for group in groups:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(group)\n",
    "        pivot = int(len(group) * ratio)\n",
    "        train += group[:pivot]\n",
    "        test += group[pivot:]\n",
    "        if debug: \n",
    "            print(f\"{group} -> {group[:pivot]}, {group[pivot:]}\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "for workload, workload_name in [(job_workload, \"JOB\")]:\n",
    "    \n",
    "    logical_trees_to_queries = defaultdict(list)\n",
    "    for q_n in workload:\n",
    "        logical_trees_to_queries[_get_logical_tree(q_n, DEFAULT_HINTSET, DEFAULT_DOP)].append(q_n)\n",
    "\n",
    "    structure_reports = []\n",
    "    for seed in range(10):\n",
    "        train, test = get_traintest_split([v for v in logical_trees_to_queries.values() if len(v) > 1], ratio=0.5, seed=seed)\n",
    "        nnmodel = NN(fit_settings=ALL_SS, inference_settings=ss, model=get_bt_regressor(\"dummy\", DEVICE))\n",
    "        nnmodel.fit(train, epochs)\n",
    "\n",
    "        for ss, ss_descr in [\n",
    "            (GREEDY_SS, \"GREEDY\"),\n",
    "            (PRUNED_GREEDY_SS, \"PRUNED GREEDY\"),\n",
    "            (LOCAL_SS, \"LOCAL\"),\n",
    "            (PRUNED_LOCAL_SS, \"PRUNED LOCAL\"),\n",
    "            (ALL_SS, \"EXHAUSTIVE\"),\n",
    "        ]:\n",
    "            structure_reports.append(get_report(nnmodel, \"NN\", train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "            structure_reports.append(get_report(nnmodel, \"NN\", test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "\n",
    "            heromodel = Hero(ss)\n",
    "            heromodel.fit(train)\n",
    "            structure_reports.append(get_report(heromodel, \"Hero\", train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "            structure_reports.append(get_report(heromodel, \"Hero\", test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "        \n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_structure_reports.json\", \"w\") as f:\n",
    "        dump(structure_reports, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(reports):\n",
    "    df = pd.DataFrame(reports)\n",
    "    df[\"ex boost (% of opt)\"] = 100 * (df[\"def_ex\"] - df[\"custom_ex\"]) / (df[\"def_ex\"] - df[\"opt_ex\"])\n",
    "    df[\"e2e boost (% of opt)\"] = 100 * (df[\"def_e2e\"] - df[\"custom_e2e\"]) / (df[\"def_e2e\"] - df[\"opt_e2e\"])\n",
    "    df[\"e2e boost (%)\"] = 100 * (df[\"def_e2e\"] - df[\"custom_e2e\"]) / df[\"def_e2e\"]\n",
    "\n",
    "    def count_real_degradations(predictions):\n",
    "        return sum(\n",
    "            _get_e2e_time(q_n, hs, dop) > 1.1 * _get_e2e_time(q_n, DEFAULT_HINTSET, DEFAULT_DOP)\n",
    "            for q_n, hs, dop in predictions\n",
    "        )\n",
    "    \n",
    "    sizes = df[\"predictions\"].apply(lambda el: len(el))\n",
    "    df[\"n_timeouts (%)\"] = 100 * df[\"n_timeouts\"].apply(lambda el: int(el)) / sizes\n",
    "    df[\"n_real_degradations (%)\"] = 100 * df[\"predictions\"].apply(count_real_degradations) / sizes\n",
    "\n",
    "    experiment_cols = [\"model\", \"workload\", \"searching_settings\"]\n",
    "    value_cols = [\n",
    "        \"e2e boost (%)\",\n",
    "        \"e2e boost (% of opt)\", \n",
    "        \"ex boost (% of opt)\", \n",
    "        \"n_timeouts (%)\", \n",
    "        \"n_real_degradations (%)\",\n",
    "        \"custom_e2e\", \n",
    "        \"custom_ex\", \n",
    "        \"custom_inference\",    \n",
    "    ]\n",
    "\n",
    "    df = df[experiment_cols + value_cols]\n",
    "\n",
    "    grouped_df = df.groupby(experiment_cols).agg(\n",
    "        {\n",
    "            col: [\"mean\", \"std\"]\n",
    "            for col in value_cols\n",
    "        }\n",
    "    ).reset_index()\n",
    "\n",
    "    def combine_mean_std(row):\n",
    "        return f\"{row['mean']:.1f} ± {row['std']:.1f}\"\n",
    "\n",
    "    for col in value_cols:\n",
    "        grouped_df[(col, 'mean ± std')] = grouped_df[col].apply(combine_mean_std, axis=1)\n",
    "\n",
    "    grouped_df.columns = [' '.join(col).strip() for col in grouped_df.columns.values]\n",
    "    columns_to_keep = experiment_cols + [f\"{col} mean\" for col in value_cols] + [f\"{col} std\" for col in value_cols]\n",
    "    grouped_df = grouped_df[columns_to_keep]\n",
    "    grouped_df.columns = experiment_cols + [f\"mean {col}\" for col in value_cols] + [f\"std {col}\" for col in value_cols]\n",
    "    return grouped_df.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ARTIFACTS_PATH}/JOB_structure_reports.json\", \"r\") as f:\n",
    "    job_structure_df = aggregate_results(load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_cols = [\n",
    "    \"model\", \n",
    "    \"searching_settings\",\n",
    "    \"mean e2e boost (%)\",\n",
    "    \"mean e2e boost (% of opt)\",\n",
    "    \"mean ex boost (% of opt)\",\n",
    "    \"mean n_timeouts (%)\",\n",
    "    \"mean n_real_degradations (%)\",    \n",
    "    \"std e2e boost (%)\",\n",
    "    \"std e2e boost (% of opt)\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_structure_df[(job_structure_df[\"workload\"] == \"JOB[train]\")].sort_values(by=\"mean e2e boost (% of opt)\", ascending=False)[interesting_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_structure_df[(job_structure_df[\"workload\"] == \"JOB[test]\")].sort_values(by=\"mean e2e boost (% of opt)\", ascending=False)[interesting_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that generalisation in the presence of structure is efficient (boost up to 36% on JOB and 70% on `SQ`), but even though the structure of the logical plans was repeated, about 20% of the predictions either slowed down the query or led them to `T/O`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "for workload, workload_name in [(job_workload, \"JOB\"), (sq_workload, \"SQ\")]:\n",
    "    random_split_reports = []\n",
    "    for seed in range(10):\n",
    "        train, test = get_traintest_split([workload], ratio=0.5, seed=seed)\n",
    "        nnmodel = NN(fit_settings=ALL_SS, inference_settings=ss, model=get_bt_regressor(\"dummy\", DEVICE))\n",
    "        nnmodel.fit(train, epochs)\n",
    "\n",
    "        for ss, ss_descr in [\n",
    "            (GREEDY_SS, \"GREEDY\"),\n",
    "            (PRUNED_GREEDY_SS, \"PRUNED GREEDY\"),\n",
    "            (LOCAL_SS, \"LOCAL\"),\n",
    "            (PRUNED_LOCAL_SS, \"PRUNED LOCAL\"),\n",
    "            (ALL_SS, \"EXHAUSTIVE\"),\n",
    "        ]:\n",
    "            random_split_reports.append(get_report(nnmodel, \"NN\", train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "            random_split_reports.append(get_report(nnmodel, \"NN\", test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "\n",
    "            heromodel = Hero(ss)\n",
    "            heromodel.fit(train)\n",
    "            random_split_reports.append(get_report(heromodel, \"Hero\", train, f\"{workload_name}[train]\", ss, ss_descr, only_def_dop=False))\n",
    "            random_split_reports.append(get_report(heromodel, \"Hero\", test, f\"{workload_name}[test]\", ss, ss_descr, only_def_dop=False))\n",
    "        \n",
    "    with open(f\"{ARTIFACTS_PATH}/{workload_name}_random_split_reports.json\", \"w\") as f:\n",
    "        dump(random_split_reports, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ARTIFACTS_PATH}/JOB_random_split_reports.json\", \"r\") as f:\n",
    "    job_random_split_df = aggregate_results(load(f))\n",
    "with open(f\"{ARTIFACTS_PATH}/SQ_random_split_reports.json\", \"r\") as f:\n",
    "    sq_random_split_df = aggregate_results(load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_random_split_df[(sq_random_split_df[\"workload\"] == \"SQ[test]\")].sort_values(by=\"mean e2e boost (%)\", ascending=False)[interesting_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_random_split_df[(job_random_split_df[\"workload\"] == \"JOB[test]\")].sort_values(by=\"mean e2e boost (%)\", ascending=False)[interesting_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.**\\\n",
    "We can clearly see that with random partitioning the \"power\" of generalisation drops significantly. We also can see, that the more plans are evaluated by the NN, the greater the probability of observing degradations and regressions (up to 30% `T/O`). But even versions with pruned search slow down queries in about 20% of cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
