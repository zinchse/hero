{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content:** This notebook provides a detailed analysis of the performance of the most successful BTCNN architecture. The primary goal is to understand the causes of its errors and to formulate hypotheses on how it generalizes its predictions.\n",
    "\n",
    "**Approach:** We begin by introducing various data features, such as whether a plan was part of the training set, whether its structure was seen in the training data, how long it takes to execute, the extent of the model's error on these plans, and so on. We then search for patterns, such as \"the error is large when certain features have specific values.\"\n",
    "\n",
    "**3D Visualization:** To gain a deeper understanding of why the model makes errors, we also perform visualizations. We identify plans with the most extreme errors, convert them into vector representations using BTCNN, project these vectors onto a two-dimensional plane, and display the top-k nearest points from the training set. This helps us understand why the training data might have been insufficient to accurately predict the label for the query being analyzed.\n",
    "\n",
    "P.S. The sections `Loading data...`, `Model and Datasets`, and `Helpers` contain a significant amount of code; feel free to skip these sections for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "BTCNN_PATH = os.path.join(ROOT_PATH, \"btcnn/src/btcnn\")\n",
    "HBO_BENCH_PATH = os.path.join(ROOT_PATH, \"hbo_bench/src/hbo_bench\")\n",
    "\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "EXPERIMENT_PATH = os.getcwd()\n",
    "ARTIFACTS_PATH = os.path.join(EXPERIMENT_PATH, \"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from json import load, dumps, dump\n",
    "from itertools import product\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from hbo_bench.utils import get_logical_tree, get_full_plan, get_selectivities, extract_list_info, preprocess\n",
    "from hbo_bench.oracle import Oracle, OracleRequest, TIMEOUT\n",
    "from hbo_bench.data_config import HINTSETS, DOPS, HINTS, DEFAULT_HINTSET, DEFAULT_DOP\n",
    "from hbo_bench.data_types import ExplainNode\n",
    "from hbo_bench.vectorization import extract_vertices_and_edges, ALL_FEATURES\n",
    "from hbo_bench.dataset import WeightedBinaryTreeDataset, weighted_binary_tree_collate\n",
    "\n",
    "from btcnn.layers import *\n",
    "from btcnn.regressor import BinaryTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zinchse/GitHub/wtf/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.interpolate import Rbf\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_oracle = Oracle(f\"{HBO_BENCH_PATH}/data/processed/JOB\")\n",
    "sq_oracle = Oracle(f\"{HBO_BENCH_PATH}/data/processed/sample_queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the order of queries in `oracle.get_query_names()` is **not fixed** (it depends on OS etc).\n",
    "So I can't reproduce `*list_info` with the same order I used in the training. Fortunately, I saved the training versions of `*list_info` and I'm going to use them.\n",
    "\n",
    "P.S. But the data itself is reproducible (the `*list_info` entities from the training and the `*list_info` generated in `_training.ipynb` are the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list_info = torch.load(f\"{ARTIFACTS_PATH}/job_list_info\", weights_only=True)\n",
    "ood_sq_list_info = torch.load(f\"{ARTIFACTS_PATH}/ood_sq_list_info\", weights_only=True)\n",
    "id_sq_list_info = torch.load(f\"{ARTIFACTS_PATH}/id_sq_list_info\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list_vertices, job_list_edges, job_list_time = [[info[key] for info in job_list_info] for key in [\"vertices\", \"edges\", \"time\"]]        \n",
    "ood_sq_list_vertices, ood_sq_list_edges, ood_sq_list_time = [[info[key] for info in ood_sq_list_info] for key in [\"vertices\", \"edges\", \"time\"]]\n",
    "id_sq_list_vertices, id_sq_list_edges, id_sq_list_time = [[info[key] for info in id_sq_list_info] for key in [\"vertices\", \"edges\", \"time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it'll be splitted correctly on train/val datasets` in `load_run`\n",
    "trainval_dataset = WeightedBinaryTreeDataset(job_list_vertices, job_list_edges, job_list_time, DEVICE)\n",
    "\n",
    "test_dataset = WeightedBinaryTreeDataset(id_sq_list_vertices, id_sq_list_edges, id_sq_list_time, DEVICE)\n",
    "ood_dataset = WeightedBinaryTreeDataset(ood_sq_list_vertices, ood_sq_list_edges, ood_sq_list_time, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Datasets\n",
    "\n",
    "We'll use in that notebook as a model the most successful architecture - Big BTCNN, Big FCNN, Instance Normalization (see `learning_dynamics.ipynb` for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = len(ALL_FEATURES)\n",
    "max_length = max([v.shape[0] for v in job_list_vertices + ood_sq_list_vertices + id_sq_list_vertices])\n",
    "batch_size = 256\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(model, ckpt_path):\n",
    "    ckpt_state = torch.load(ckpt_path, map_location=DEVICE, weights_only=True)\n",
    "    model.load_state_dict(ckpt_state['model_state_dict'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer.load_state_dict(ckpt_state['optimizer_state_dict'])\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "    scheduler.load_state_dict(ckpt_state['scheduler_state_dict'])\n",
    "    start_epoch = ckpt_state[\"epoch\"]    \n",
    "    return model, optimizer, scheduler, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_btcnn_and_instance_norm = lambda: BinaryTreeSequential(\n",
    "    BinaryTreeConv(in_channels, 64),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(64, 128),\n",
    "    BinaryTreeInstanceNorm(128),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(128, 256),\n",
    "    BinaryTreeInstanceNorm(256),\n",
    "    BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "    BinaryTreeConv(256, 512),\n",
    "    BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1)),\n",
    ")\n",
    "\n",
    "big_fcnn = lambda: torch.nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Softplus(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_run(run):\n",
    "    generator = torch.Generator().manual_seed(42+run-1)\n",
    "    train_dataset, val_dataset = torch.utils.data.dataset.random_split(trainval_dataset, [0.8, 0.2], generator=generator)\n",
    "    train_dataloader, val_dataloader = [\n",
    "        DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda el: weighted_binary_tree_collate(el, max_length),\n",
    "            drop_last=False\n",
    "        )\n",
    "        for dataset in [train_dataset, val_dataset]\n",
    "    ] \n",
    "\n",
    "    model, optimizer, scheduler, start_epoch = load_ckpt(\n",
    "        model=BinaryTreeRegressor(big_btcnn_and_instance_norm(), big_fcnn()),\n",
    "        ckpt_path=f\"{EXPERIMENT_PATH}/models/BigBTCNN_BigFCNN_InstanceNorm_{run}.pth\",\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    return model, train_dataset, val_dataset, train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(model, dataset):\n",
    "    with torch.no_grad():\n",
    "        running_loss, total_samples = .0, 0\n",
    "        for v, e, f, t in tqdm(dataset):\n",
    "            v, e = preprocess(v, e)               \n",
    "            running_loss += f.to(\"cpu\") * ((model(v.unsqueeze(0), e.unsqueeze(0)).squeeze(0) - t).to(\"cpu\") ** 2).float()\n",
    "            total_samples += f.to(\"cpu\")\n",
    "    print(total_samples)\n",
    "    return (running_loss / total_samples).item() if total_samples else .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_dataset, val_dataset, train_dataloader, val_dataloader = load_run(1)\n",
    "print(f\"Expected train loss is ~0.06939, real one is {get_mse(model, train_dataset):0.4f}\")\n",
    "print(f\"Expected val loss is 27.72444, real one is {get_mse(model, val_dataset):0.4f}\")\n",
    "print(\"It seems that model loading is sucessfully finished! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers\n",
    "\n",
    "There are just a lot of auxilary functions, that part can be skipped for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure(v, e):\n",
    "    \"\"\"cleans cards (and selectivities) and returns hashable repr\"\"\"\n",
    "    scale = 10 ** 3\n",
    "    v = v.clone()\n",
    "    v = torch.round(v * scale) / scale\n",
    "    v[-2:,:] = 0\n",
    "    return str(v.flatten().tolist()), str(e.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree(v, e):\n",
    "    \"\"\"returns hashable repr for v, e\"\"\"\n",
    "    v = v.clone()\n",
    "    return str(v.flatten().tolist()), str(e.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_dataset(dataset, model, train_structures, train_trees, data_type):\n",
    "    df = pd.DataFrame(list(dataset), columns=[\"vertices\", \"edges\", \"frequency\", \"time\"])\n",
    "    df[\"data_type\"] = data_type\n",
    "\n",
    "    df[\"time_category\"] = \"small\"\n",
    "    df.loc[df[\"time\"] > 0.2, \"time_category\"] = \"medium\"\n",
    "    df.loc[df[\"time\"] > 4, \"time_category\"] = \"big\"\n",
    "\n",
    "    df[\"structure\"] = df.apply(lambda row: get_structure(*preprocess(row[\"vertices\"], row[\"edges\"])), axis=1)\n",
    "    df[\"structure_category\"] = \"unseen\"\n",
    "    df.loc[df[\"structure\"].isin(train_structures), \"structure_category\"] = \"seen\"\n",
    "\n",
    "    df[\"tree\"] = df.apply(lambda row: get_tree(*preprocess(row[\"vertices\"], row[\"edges\"])), axis=1)\n",
    "    df[\"tree_category\"] = \"unseen\"\n",
    "    df.loc[df[\"tree\"].isin(train_trees), \"tree_category\"] = \"seen\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        df[\"embedding\"] = df.apply(lambda row: model.btcnn(*[t.unsqueeze(0) for t in preprocess(row[\"vertices\"], row[\"edges\"])]).squeeze(0).to(\"cpu\"), axis=1)\n",
    "        df[\"prediction\"] = df.apply(lambda row: model(*[t.unsqueeze(0) for t in preprocess(row[\"vertices\"], row[\"edges\"])]).to(\"cpu\"), axis=1)        \n",
    "\n",
    "    for col in [\"prediction\", \"frequency\", \"time\"]:\n",
    "        df[col] = df.apply(lambda row: row[col].item(), axis=1)\n",
    "\n",
    "    df[\"error\"] = df[\"prediction\"] - df[\"time\"]\n",
    "    df[\"prediction_category\"] = \"underestimated\"\n",
    "    df.loc[df[\"error\"] > 0, \"prediction_category\"] = \"overestimated\"\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, data_type_predicate, structure_predicate=\"all\", tree_predicate=\"all\", time_predicate=\"all\", prediction_predicate=\"all\"):\n",
    "    idx = (df[\"data_type\"] == data_type_predicate)\n",
    "    if structure_predicate != \"all\":\n",
    "        idx = idx & (df[\"structure_category\"] == structure_predicate)\n",
    "    if tree_predicate != \"all\":\n",
    "        idx = idx & (df[\"tree_category\"] == tree_predicate)        \n",
    "    if time_predicate != \"all\":\n",
    "        idx = idx & (df[\"time_category\"] == time_predicate)\n",
    "    if prediction_predicate != \"all\":\n",
    "        idx = idx & (df[\"prediction_category\"] == prediction_predicate)\n",
    "    return df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_run_and_get_filtered_df(run, data_type_predicate, structure_predicate, tree_predicate, time_predicate, prediction_predicate):\n",
    "    model, train_dataset, val_dataset, train_dataloader, val_dataloader = load_run(run)\n",
    "    train_structures = {get_structure(*preprocess(v, e)) for v, e, *_ in train_dataset}\n",
    "    train_trees = {get_tree(*preprocess(v, e)) for v, e, *_ in train_dataset}\n",
    "    datasets = (\n",
    "        featurize_dataset(dataset, model, train_structures, train_trees, data_type) \n",
    "        for dataset, data_type in zip([train_dataset, val_dataset, test_dataset, ood_dataset], [\"train\", \"val\", \"test\", \"ood\"])\n",
    "    )\n",
    "    df = pd.concat(datasets, ignore_index=True)\n",
    "    return filter_df(df, data_type_predicate, structure_predicate, tree_predicate, time_predicate, prediction_predicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## '$\\langle \\text{vertices}, \\text{edges} \\rangle \\rightarrow \\text{something}$'-like maps\n",
    "\n",
    "In future we will need to know time corresponding particular pair `(vertices, edges)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VE_TO_TIMES = defaultdict(set)\n",
    "TRAIN_VET_TO_QUERY_INFO = defaultdict(set)\n",
    "\n",
    "for info in job_list_info:\n",
    "    v, e, t = info[\"vertices\"], info[\"edges\"], info[\"time\"]\n",
    "    v, e = preprocess(v, e)\n",
    "\n",
    "    ve = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "    TRAIN_VE_TO_TIMES[ve].add(t)\n",
    "\n",
    "    vet = str(v.flatten().tolist()), str(e.flatten().tolist()), str(t.flatten().tolist())\n",
    "    TRAIN_VET_TO_QUERY_INFO[vet].add((info[\"query_name\"], info[\"hintset\"], info[\"dop\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOD_VE_TO_TIMES = defaultdict(set)\n",
    "OOD_VET_TO_QUERY_INFO = defaultdict(set)\n",
    "\n",
    "for info in ood_sq_list_info:\n",
    "    v, e, t = info[\"vertices\"], info[\"edges\"], info[\"time\"]\n",
    "    v, e = preprocess(v, e)\n",
    "\n",
    "    ve = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "    OOD_VE_TO_TIMES[ve].add(t)\n",
    "\n",
    "    vet = str(v.flatten().tolist()), str(e.flatten().tolist()), str(t.flatten().tolist())\n",
    "    OOD_VET_TO_QUERY_INFO[vet].add((info[\"query_name\"], info[\"hintset\"], info[\"dop\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_VE_TO_COSTS = defaultdict(list)\n",
    "JOB_VE_TO_TIMES = defaultdict(list)\n",
    "for info in job_list_info:\n",
    "    v, e, t = info[\"vertices\"], info[\"edges\"], info[\"time\"]\n",
    "    v, e = preprocess(v, e)\n",
    "    ve = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "    cost = job_oracle.get_cost(OracleRequest(query_name=info[\"query_name\"], hintset=info[\"hintset\"], dop=info[\"dop\"]))\n",
    "    JOB_VE_TO_COSTS[ve].append(cost)\n",
    "    JOB_VE_TO_TIMES[ve].append(t.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQ_VE_TO_COSTS = defaultdict(list)\n",
    "SQ_VE_TO_TIMES = defaultdict(list)\n",
    "for info in id_sq_list_info + ood_sq_list_info:\n",
    "    v, e, t = info[\"vertices\"], info[\"edges\"], info[\"time\"]\n",
    "    v, e = preprocess(v, e)\n",
    "    ve = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "    cost = sq_oracle.get_cost(OracleRequest(query_name=info[\"query_name\"], hintset=info[\"hintset\"], dop=info[\"dop\"]))\n",
    "    SQ_VE_TO_COSTS[ve].append(cost)\n",
    "    SQ_VE_TO_TIMES[ve].append(t.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_info(v, e, ve_map, vet_map, oracle):\n",
    "    ve = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "    assert ve in ve_map\n",
    "    res = []\n",
    "    for t in set(ve_map[ve]):\n",
    "        vet = str(v.flatten().tolist()), str(e.flatten().tolist()), str(t.flatten().tolist())\n",
    "        for q_n, hs, dop in vet_map[vet]:\n",
    "            res.append((q_n, hs, dop, oracle.get_execution_time(OracleRequest(query_name=q_n, hintset=hs, dop=dop))))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dirty(v, e):\n",
    "    return TIMEOUT in [t for *_, t in get_query_info(v, e, TRAIN_VE_TO_TIMES, TRAIN_VET_TO_QUERY_INFO, job_oracle)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhoods extracting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighborhoods(v, e, target_d, debug=False):\n",
    "    \"\"\"collects `target_d`-neighborhoods - list its subtrees of height `target_d`\"\"\"\n",
    "    neighborhoods = []\n",
    "\n",
    "    def recurse(root, cur_d):\n",
    "        ngbs = [root]\n",
    "        real_d = -float(\"inf\")\n",
    "        if cur_d == target_d:\n",
    "            return 0, ngbs\n",
    "        for child_d, child_ngbs in [recurse(child.item()-1, cur_d+1) for child in e[root][1:] if child]:\n",
    "            ngbs += child_ngbs\n",
    "            real_d = max(real_d, child_d)\n",
    "        return real_d + 1, ngbs\n",
    "\n",
    "    neighborhoods = [ngbs for i in range(len(v)) for (max_d, ngbs) in [recurse(i, 0)] if max_d == target_d]\n",
    "    \n",
    "    if debug:\n",
    "        return neighborhoods\n",
    "\n",
    "    return [torch.stack([v[i] for i in l if i]) for l in neighborhoods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportions_of_subtree_matches(dataset, subtrees_height, seen_subtrees):\n",
    "    matches = []\n",
    "    for v, e, f, t in dataset:\n",
    "        per_v_matches = []\n",
    "        for ngb in get_neighborhoods(v, e, subtrees_height):\n",
    "            per_v_matches.append(str(ngb.detach().to(\"cpu\").tolist()) in seen_subtrees)\n",
    "        matches.append(per_v_matches)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** Tree (with taking into account the padding vertex)\n",
    "```python3\n",
    "                     1                      \n",
    "                     |                      \n",
    "                     2                      \n",
    "                    /  \\                 \n",
    "                   3   17               \n",
    "                  / \\                       \n",
    "                 4  16                      \n",
    "                / \\  \n",
    "               5  15                            \n",
    "              / \\                               \n",
    "             /   \\\n",
    "            /     \\                                  \n",
    "           6       13                                 \n",
    "          / \\       |                                \n",
    "         7  12     14                               \n",
    "        / \\                                          \n",
    "       8  11                                       \n",
    "      / \\                                     \n",
    "     9  10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Edges in example tree:\")\n",
    "for p, c1, c2 in job_list_edges[0]:\n",
    "    if c1 and c2:\n",
    "        print(f\"{p.item()} -> [{c1.item()}, {c2.item()}]\")\n",
    "    elif c1:\n",
    "        print(f\"{p.item()} -> [{c1.item()}]\")\n",
    "    elif c2:\n",
    "        print(f\"{p.item()} -> [{c2.item()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected results: the only  difference is that there's no 0\\nReal results:\")\n",
    "for edge, ngbs in list(zip([x.tolist() for x in job_list_edges[0]], get_neighborhoods(job_list_vertices[0], job_list_edges[0], 1, debug=True)))[:5]:\n",
    "    print(f\"{[el +1 for el in ngbs]} vs {edge}\")\n",
    "print(\"It works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childs = defaultdict(list)\n",
    "for parent, child1, child2 in job_list_edges[0]:\n",
    "    if child1:\n",
    "        childs[parent.item()-1].append(child1.item()-1)\n",
    "    if child2:\n",
    "        childs[parent.item()-1].append(child2.item()-1)        \n",
    "        \n",
    "for ngbs_1d, ngbs_2d in list(zip(get_neighborhoods(job_list_vertices[0], job_list_edges[0], 1, debug=True), get_neighborhoods(job_list_vertices[0], job_list_edges[0], 2, debug=True)))[:5]:\n",
    "    print(f\"{[el+1 for el in ngbs_1d]} -> {[el + 1 for el in ngbs_2d]}\")\n",
    "print(\"Thank God it's working too...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neighborhoods_dict(n_runs=5):\n",
    "    res = defaultdict(list)\n",
    "    for run in range(1, n_runs+1):\n",
    "        _, train_dataset, val_dataset, *_ = load_run(run)\n",
    "        for d in range(1, 5):\n",
    "            train_subtrees = set()\n",
    "            for v, e, f, t in train_dataset:\n",
    "                for ngb in get_neighborhoods(v, e, d):\n",
    "                    subtree = str(ngb.detach().to(\"cpu\").tolist())\n",
    "                    train_subtrees.add(subtree)\n",
    "\n",
    "            val_l = get_proportions_of_subtree_matches(val_dataset, d, train_subtrees)\n",
    "            val_l = [int(el) for subl in val_l for el in subl]\n",
    "            res[(d, \"val\")].append(val_l)\n",
    "\n",
    "            test_l = get_proportions_of_subtree_matches(test_dataset, d, train_subtrees)\n",
    "            test_l = [int(el) for subl in test_l for el in subl]\n",
    "            res[(d, \"test\")].append(test_l)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_neighborhoods_dict(neighborhoods_dict, runs_range, save=True):\n",
    "    pm = u\"\\u00B1\"\n",
    "    aggregated_neighborhoods_list = []\n",
    "    \n",
    "    for key in neighborhoods_dict:\n",
    "        val = neighborhoods_dict[key]\n",
    "        aggregated_neighborhoods_list.append((*key, f\"{np.mean([np.mean(val[i-1]) for i in runs_range]):0.3f}{pm}{np.std([np.mean(val[i-1]) for i in runs_range]):0.3f}\"))\n",
    "    aggregated_neighborhoods_df = pd.DataFrame(aggregated_neighborhoods_list, columns=[\"height\", \"data_type\", \"match_probability\"])\n",
    "    \n",
    "    if save:\n",
    "        aggregated_neighborhoods_df.to_csv(f\"{ARTIFACTS_PATH}/aggregated_neighborhoods_df.csv\", index=False)\n",
    "    \n",
    "    return aggregated_neighborhoods_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_dict = calculate_neighborhoods_dict(5)\n",
    "aggregated_neighborhoods_df = aggregate_neighborhoods_dict(neighborhoods_dict, runs_range=range(1, 5+1), save=True)\n",
    "# aggregated_neighborhoods_df = pd.read_csv(f\"{ARTIFACTS_PATH}/aggregated_neighborhoods_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stratified_metrics(n_runs=5):\n",
    "    res = defaultdict(list)\n",
    "\n",
    "    for run in range(1, n_runs+1):\n",
    "        model, train_dataset, val_dataset, train_dataloader, val_dataloader = load_run(run)\n",
    "        train_structures = {get_structure(*preprocess(v, e)) for v, e, *_ in train_dataset}\n",
    "        train_trees = {get_tree(*preprocess(v, e)) for v, e, *_ in train_dataset}\n",
    "        \n",
    "        datasets = (\n",
    "            featurize_dataset(dataset, model, train_structures, train_trees, data_type) \n",
    "            for dataset, data_type in zip([train_dataset, val_dataset, test_dataset, ood_dataset], [\"train\", \"val\", \"test\", \"ood\"])\n",
    "        )\n",
    "        df = pd.concat(datasets, ignore_index=True)\n",
    "        \n",
    "        for data_type_predicate in [\"train\", \"val\", \"test\", \"ood\"]:\n",
    "            for structure_predicate in [\"all\", \"seen\", \"unseen\"]:\n",
    "                for tree_predicate in [\"all\", \"seen\", \"unseen\"]:\n",
    "                    for time_predicate in [\"all\", \"small\", \"medium\", \"big\"]:\n",
    "                        for prediction_predicate in [\"all\", \"underestimated\", \"overestimated\"]:\n",
    "                            filtered_df = filter_df(df, data_type_predicate, structure_predicate, tree_predicate, time_predicate, prediction_predicate)\n",
    "                            size = int(filtered_df[\"frequency\"].sum())\n",
    "                            mse = float((((filtered_df[\"prediction\"] - filtered_df[\"time\"]) ** 2) * filtered_df[\"frequency\"]).sum() / size) if size else .0\n",
    "                            mape = float((((filtered_df[\"prediction\"] - filtered_df[\"time\"]).abs() / filtered_df[\"time\"]) * filtered_df[\"frequency\"]).sum() / size) if size else .0\n",
    "                            res[(data_type_predicate, structure_predicate, tree_predicate, time_predicate, prediction_predicate)].append((size, mse, mape))\n",
    "                            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metrics_dict(metrics_dict, runs_range, save=True):\n",
    "    pm = u\"\\u00B1\"\n",
    "    aggregated_metrics_list = []\n",
    "    for key in metrics_dict:\n",
    "        val = metrics_dict[key]\n",
    "        aggregated_metrics_list.append(\n",
    "            (\n",
    "                *key,\n",
    "                f\"{np.mean([val[i-1][0] for i in runs_range]):0.0f}\",\n",
    "                f\"{np.mean([val[i-1][1] for i in runs_range]):0.3f}{pm}{np.std([val[i-1][1] for i in runs_range]):0.3f}\", \n",
    "                f\"{np.mean([val[i-1][2] for i in runs_range]):0.3f}{pm}{np.std([val[i-1][2] for i in runs_range]):0.3f}\",\n",
    "            )\n",
    "        )\n",
    "    aggregated_metrics_df = pd.DataFrame(\n",
    "        aggregated_metrics_list, \n",
    "        columns=[\"data_type\", \"structure_category\", \"tree_category\", \"time_category\", \"prediction_category\", \"size\", \"mse\", \"mape\"]\n",
    "    )\n",
    "    if save:\n",
    "        aggregated_metrics_df.to_csv(f\"{ARTIFACTS_PATH}/aggregated_metrics_df.csv\", index=False)\n",
    "    return aggregated_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_metrics_df(df, allowed_data_type_predicates=None, allowed_structure_predicates=None, allowed_tree_predicates=None, allowed_time_predicates=None, allowed_prediction_predicates=None):\n",
    "    if allowed_data_type_predicates is None:\n",
    "        allowed_data_type_predicates = [\"train\", \"val\", \"test\", \"ood\"]\n",
    "    if allowed_structure_predicates is None:\n",
    "        allowed_structure_predicates = [\"seen\", \"unseen\", \"all\"]        \n",
    "    if allowed_tree_predicates is None:\n",
    "        allowed_tree_predicates = [\"seen\", \"unseen\", \"all\"]                \n",
    "    if allowed_time_predicates is None:\n",
    "        allowed_time_predicates = [\"big\", \"medium\", \"small\", \"all\"]                \n",
    "    if allowed_prediction_predicates is None:\n",
    "        allowed_prediction_predicates = [\"underestimated\", \"overestimated\", \"all\"]                \n",
    "\n",
    "    idx = (df[\"data_type\"].isin(allowed_data_type_predicates))\n",
    "    idx = idx & (df[\"structure_category\"].isin(allowed_structure_predicates))\n",
    "    idx = idx & (df[\"tree_category\"].isin(allowed_tree_predicates))\n",
    "    idx = idx & (df[\"time_category\"].isin(allowed_time_predicates))\n",
    "    idx = idx & (df[\"prediction_category\"].isin(allowed_prediction_predicates))\n",
    "\n",
    "    return df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = calculate_stratified_metrics(5)\n",
    "aggregated_metrics_df = aggregate_metrics_dict(metrics_dict, range(1, 5+1), True)\n",
    "# aggregated_metrics_df = pd.read_csv(f\"{ARTIFACTS_PATH}/aggregated_metrics_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_knn_times(\n",
    "    support_points,\n",
    "    figsize=(10, 4),\n",
    "    label_fontsize=15, \n",
    "    linewidth=2, \n",
    "    markersize=6, \n",
    "    xlabel=\"k\", \n",
    "    ylabel=\"Average time\", \n",
    "    tick_label_fontsize=15, \n",
    "    save_name=None    \n",
    "):\n",
    "\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_palette(\"deep\")\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    all_t, all_f = 0, 0\n",
    "    res = []\n",
    "    for d, v, e, f, t, p, emb in sorted(support_points, key=lambda x: x[0]):\n",
    "        all_f += f\n",
    "        all_t += f * t\n",
    "        res.append(all_t / all_f)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.2)\n",
    "    plt.plot(res, marker='o', markersize=markersize, linewidth=linewidth, linestyle='-')\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Average Time (sec)\")\n",
    "    if save_name:\n",
    "        plt.savefig(f\"{ARTIFACTS_PATH}/{save_name}\", format='svg', dpi=300)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_support_points_and_all_distances(target_embedding, points_dataloader, k):\n",
    "    top_k = []\n",
    "    target_embedding = target_embedding.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        all_distances = []\n",
    "        for (vertices_batch, edges_batch, f_batch), t_batch in tqdm(points_dataloader):    \n",
    "            embeddings = model.btcnn(vertices_batch.to(DEVICE), edges_batch.to(DEVICE))\n",
    "            predicts = model(vertices_batch, edges_batch).squeeze(0)\n",
    "            distances = torch.cdist(target_embedding.unsqueeze(0), embeddings, p=2).squeeze(0) / torch.norm(target_embedding)\n",
    "            \n",
    "            for i in range(predicts.size(0)):\n",
    "                distance = distances[i].item()\n",
    "                embedding = embeddings[i]\n",
    "                all_distances.append(distance)\n",
    "                v, e, f, t, predict = vertices_batch[i], edges_batch[i], f_batch[i].item(), t_batch[i].item(), predicts[i].item()\n",
    "                \n",
    "                # id is here to avoid problems with comparing tensors\n",
    "                heapq.heappush(top_k, (-distance, id(v), v.to(\"cpu\"), e.to(\"cpu\"), f, t, predict, embedding))\n",
    "                while len(top_k) > k:\n",
    "                    heapq.heappop(top_k)\n",
    "                    \n",
    "    return [(-d, v, e, f, t, p, emb) for (d, t_id, v, e, f, t, p, emb) in top_k], all_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_args_for_3d_plot(idx, interesting_subdf, n_support_points=100, neighborhoods_size=50, noise_level=0.1):\n",
    "    target_v, target_e, target_embedding, target_y = interesting_subdf.loc[idx][[\"vertices\", \"edges\", \"embedding\", \"time\"]]\n",
    "\n",
    "    support_points, all_distances = get_k_support_points_and_all_distances(target_embedding, train_dataloader, k=n_support_points)\n",
    "    main_X = [emb.to(\"cpu\") for *_, emb in support_points]\n",
    "    main_y = [t for *_, t, p, emb in support_points]\n",
    "\n",
    "    all_auxilary_X, all_auxilary_y = [], []\n",
    "    for d, v, e, *_ in support_points:\n",
    "        for noise_level_ in [noise_level/10, noise_level, noise_level*10]:\n",
    "            # data from dataloader is already preprocessed\n",
    "            auxilary_X, auxilary_y = get_close_embeddings(v, e, model, neighborhoods_size, noise_level_, do_preprocess=False)\n",
    "            all_auxilary_X += auxilary_X\n",
    "            all_auxilary_y += auxilary_y\n",
    "\n",
    "    for noise_level_ in [noise_level/10, noise_level, noise_level*10]:\n",
    "        auxilary_X, auxilary_y = get_close_embeddings(target_v, target_e, model, neighborhoods_size, noise_level_)\n",
    "        all_auxilary_X += auxilary_X\n",
    "        all_auxilary_y += auxilary_y\n",
    "        \n",
    "    return main_X, main_y, target_embedding, target_y, all_auxilary_X, all_auxilary_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_surface(\n",
    "    main_x, \n",
    "    main_y, \n",
    "    target_x, \n",
    "    target_y, \n",
    "    auxilary_x, \n",
    "    auxilary_y, \n",
    "    figsize=(12, 6), \n",
    "    save_name=None, \n",
    "    elev=0,\n",
    "    azim=0, \n",
    "    softplus_rescale=False, \n",
    "    with_auxilary_points=False, \n",
    "    projector=umap.UMAP,\n",
    "    label_fontsize=12, \n",
    "    title_fontsize=14, \n",
    "    tick_label_fontsize=10, \n",
    "    legend_fontsize=12, \n",
    "    cbar_label_fontsize=12,\n",
    "):\n",
    "    projector = projector(n_components=2)\n",
    "    projector.fit(torch.stack(main_x).numpy())\n",
    "    all_X_2d = projector.transform(torch.stack(main_x + auxilary_x).numpy())\n",
    "    \n",
    "    if softplus_rescale:\n",
    "        main_y = np.log(np.exp(main_y) - 1).tolist()\n",
    "        target_y = np.log(np.exp(target_y) - 1)\n",
    "        auxilary_y = np.log(np.exp(auxilary_y) - 1).tolist()\n",
    "\n",
    "    all_y = main_y + auxilary_y\n",
    "    main_X_2d = projector.transform(torch.stack(main_x).numpy())\n",
    "    auxilary_X_2d = projector.transform(torch.stack(auxilary_x).numpy())\n",
    "    target_x_2d = projector.transform(torch.stack([target_x]).numpy())[0]\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_palette(\"deep\")\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plt.rcParams['text.usetex'] = False\n",
    "    \n",
    "    rbf = Rbf(all_X_2d[:, 0], all_X_2d[:, 1], all_y, function='linear')\n",
    "    x_ = np.linspace(np.min(all_X_2d[:, 0]), np.max(all_X_2d[:, 0]), 100)\n",
    "    y_ = np.linspace(np.min(all_X_2d[:, 1]), np.max(all_X_2d[:, 1]), 100)\n",
    "    X_grid, Y_grid = np.meshgrid(x_, y_)\n",
    "    Z_grid = rbf(X_grid, Y_grid)\n",
    "    \n",
    "    if with_auxilary_points:\n",
    "        ax.scatter(auxilary_X_2d[:, 0], auxilary_X_2d[:, 1], auxilary_y, color='black', s=1, marker='.', label='Sampled Point', zorder=10, alpha=0.7)\n",
    "    \n",
    "    ax.scatter(main_X_2d[:, 0], main_X_2d[:, 1], main_y, color='black', s=75, label='Train Point', marker='.', alpha=1, zorder=-2)\n",
    "    ax.scatter(target_x_2d[0], target_x_2d[1], target_y, color='red', s=100, marker='o', label='Target Point', zorder=10000, alpha=1, edgecolors='r')\n",
    "\n",
    "    surface = ax.plot_surface(X_grid, Y_grid, Z_grid, cmap='viridis', edgecolor='none', alpha=0.7, zorder=1)\n",
    "    cbar = fig.colorbar(surface, ax=ax, shrink=1, aspect=20)\n",
    "    cbar.set_label('Value', fontsize=cbar_label_fontsize)\n",
    "\n",
    "    ax.legend(fontsize=legend_fontsize)\n",
    "    ax.set_xlabel('Component #1', fontsize=label_fontsize,)\n",
    "    ax.set_ylabel('Component #2', fontsize=label_fontsize)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])    \n",
    "\n",
    "    if save_name:\n",
    "        plt.savefig(f\"{ARTIFACTS_PATH}/{save_name}\", format=\"svg\", bbox_inches=\"tight\", dpi=300)\n",
    "        \n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_close_embeddings(v, e, model, n=100, noise_level=0.1, do_preprocess=True):\n",
    "    v, e = preprocess(v, e) if do_preprocess else (v, e)\n",
    "    v, e, model = v.to(\"cpu\"), e.to(\"cpu\"), model.to(\"cpu\")\n",
    "\n",
    "    noise = 1 + (torch.randn(n, *v.shape) * noise_level)\n",
    "    noised_v = noise * v\n",
    "    noised_e = e.unsqueeze(0).expand((n, -1, -1, -1))\n",
    "    noised_predictions = model(noised_v, noised_e)\n",
    "    noised_embeddings = model.btcnn(noised_v, noised_e)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    return [emb.detach() for emb in noised_embeddings], [el.item() for el in noised_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_times(train_times, test_times, val_times, ood_times, fontsize=12, figsize=(12, 6), save_name=None):\n",
    "    plt.rcParams.update({'font.size': fontsize})\n",
    "    fig, axs = plt.subplots(4, 1, figsize=figsize, sharex=True, constrained_layout=True) #sharey=True, \n",
    "\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_palette(\"deep\")\n",
    "\n",
    "    axs[0].set_xscale('log')\n",
    "    sns.histplot(train_times, kde=True, ax=axs[0])\n",
    "    axs[0].set_title('Train')\n",
    "    #axs[0].set_xlabel('Time (sec)', fontsize=fontsize)\n",
    "    axs[0].set_ylabel('', fontsize=fontsize)\n",
    "    axs[0].tick_params(axis='x', which='both', labelbottom=True)\n",
    "    axs[0].set_yticklabels([])\n",
    "\n",
    "    axs[1].set_xscale('log')\n",
    "    sns.histplot(val_times, kde=True, ax=axs[1])\n",
    "    axs[1].set_title('Validation')\n",
    "    #axs[1].set_xlabel('Time (sec)', fontsize=fontsize)\n",
    "    axs[1].set_ylabel('', fontsize=fontsize)\n",
    "    axs[1].tick_params(axis='x', which='both', labelbottom=True)\n",
    "    axs[1].set_yticklabels([])\n",
    "\n",
    "    axs[2].set_xscale('log')\n",
    "    sns.histplot(test_times, kde=True, ax=axs[2])\n",
    "    axs[2].set_title('Test')\n",
    "    #axs[2].set_xlabel('Time (sec)', fontsize=fontsize)\n",
    "    axs[2].set_ylabel('', fontsize=fontsize)\n",
    "    axs[2].tick_params(axis='x', which='both', labelbottom=True)\n",
    "    axs[2].set_yticklabels([])\n",
    "\n",
    "    axs[3].set_xscale('log')\n",
    "    sns.histplot(ood_times, kde=True, ax=axs[3])\n",
    "    axs[3].set_title('OOD')\n",
    "    axs[3].set_xlabel('Time (sec)', fontsize=fontsize)\n",
    "    axs[3].set_ylabel('', fontsize=fontsize)\n",
    "    axs[3].tick_params(axis='x', which='both', labelbottom=True, labelsize=fontsize+3)\n",
    "    axs[3].set_yticklabels([])\n",
    "\n",
    "    if save_name:\n",
    "        plt.savefig(f\"{ARTIFACTS_PATH}/{save_name}\", format=\"svg\", dpi=300)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Ranking Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featurized_df(run):\n",
    "    model, train_dataset, val_dataset, train_dataloader, val_dataloader = load_run(run=run)\n",
    "\n",
    "    train_structures = {get_structure(*preprocess(v, e)) for v, e, *_ in train_dataset}\n",
    "    train_trees = {get_tree(*preprocess(v, e)) for v, e, *_ in train_dataset}\n",
    "\n",
    "    datasets = (\n",
    "        featurize_dataset(dataset, model, train_structures, train_trees, data_type) \n",
    "        for dataset, data_type in zip([train_dataset, val_dataset, test_dataset, ood_dataset], [\"train\", \"val\", \"test\", \"ood\"])\n",
    "    )\n",
    "    return pd.concat(datasets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlations(df):\n",
    "    costs_map = JOB_VE_TO_COSTS if \"train\" in df[\"data_type\"].values or \"val\" in df[\"data_type\"].values else SQ_VE_TO_COSTS\n",
    "    times_map = JOB_VE_TO_TIMES if \"train\" in df[\"data_type\"].values or \"val\" in df[\"data_type\"].values else SQ_VE_TO_TIMES\n",
    "    \n",
    "    costs, times, predictions = [], [], []\n",
    "    for _, (v, e, p) in df[[\"vertices\", \"edges\", \"prediction\"]].iterrows():\n",
    "        v, e = preprocess(v, e)\n",
    "        ve = str(v.flatten().tolist()), str(e.flatten().tolist())\n",
    "        assert ve in costs_map and ve in times_map and len(costs_map[ve]) == len(times_map[ve])\n",
    "        for cost, time in zip(costs_map[ve], times_map[ve]):\n",
    "            costs.append(cost), times.append(time), predictions.append(p)    \n",
    "    return kendalltau(times, costs)[0], kendalltau(times, predictions)[0], spearmanr(times, costs)[0], spearmanr(times, predictions)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_to_row(correlations):\n",
    "    pm = u\"\\u00B1\"\n",
    "    return f\"{np.nanmean(correlations):0.3f}{pm}{np.nanstd(correlations):0.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stratified_correlations(n_runs=5):\n",
    "    res = defaultdict(list)\n",
    "    for run in range(1, n_runs+1):\n",
    "        df = get_featurized_df(run=run)\n",
    "        parameters = product(\n",
    "            [\"train\", \"val\", \"test\", \"ood\"],\n",
    "            [\"all\", \"seen\", \"unseen\"],\n",
    "            [\"all\", \"seen\", \"unseen\"],\n",
    "            [\"all\", \"small\", \"medium\", \"big\"],\n",
    "            [\"all\", \"underestimated\", \"overestimated\"]\n",
    "        )  \n",
    "        for data_type, structure, tree, time, prediction in parameters:\n",
    "            filtered_df = filter_df(df, **{\n",
    "                \"data_type_predicate\": data_type,\n",
    "                \"structure_predicate\": structure,\n",
    "                \"tree_predicate\": tree,\n",
    "                \"time_predicate\": time,\n",
    "                \"prediction_predicate\": prediction\n",
    "            })        \n",
    "            size = int(filtered_df[\"frequency\"].sum())\n",
    "            correlations = get_correlations(filtered_df)\n",
    "            res[(data_type, structure, tree, time, prediction)].append((size, *correlations))                         \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subdf(df, allowed_data_type_predicates=None, allowed_structure_predicates=None, allowed_tree_predicates=None, allowed_time_predicates=None, allowed_prediction_predicates=None):\n",
    "    if allowed_data_type_predicates is None:\n",
    "        allowed_data_type_predicates = [\"train\", \"val\", \"test\", \"ood\"]\n",
    "    if allowed_structure_predicates is None:\n",
    "        allowed_structure_predicates = [\"seen\", \"unseen\", \"all\"]        \n",
    "    if allowed_tree_predicates is None:\n",
    "        allowed_tree_predicates = [\"seen\", \"unseen\", \"all\"]                \n",
    "    if allowed_time_predicates is None:\n",
    "        allowed_time_predicates = [\"big\", \"medium\", \"small\", \"all\"]                \n",
    "    if allowed_prediction_predicates is None:\n",
    "        allowed_prediction_predicates = [\"underestimated\", \"overestimated\", \"all\"]                \n",
    "\n",
    "    idx = (df[\"data_type\"].isin(allowed_data_type_predicates))\n",
    "    idx = idx & (df[\"structure_category\"].isin(allowed_structure_predicates))\n",
    "    idx = idx & (df[\"tree_category\"].isin(allowed_tree_predicates))\n",
    "    idx = idx & (df[\"time_category\"].isin(allowed_time_predicates))\n",
    "    idx = idx & (df[\"prediction_category\"].isin(allowed_prediction_predicates))\n",
    "\n",
    "    return df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_correlations_dict(correlations_dict, runs_range, save=True):\n",
    "    aggregated_list = [\n",
    "        (\n",
    "            data_type, structure, tree, time, prediction, \n",
    "            int(np.mean([correlations_dict[key][run-1][0] for run in runs_range])), \n",
    "            correlations_to_row([correlations_dict[key][run-1][1] for run in runs_range]),\n",
    "            correlations_to_row([correlations_dict[key][run-1][2] for run in runs_range]),\n",
    "            correlations_to_row([correlations_dict[key][run-1][3] for run in runs_range]),\n",
    "            correlations_to_row([correlations_dict[key][run-1][4] for run in runs_range])\n",
    "        )\n",
    "        for key in correlations_dict\n",
    "        for data_type, structure, tree, time, prediction in [key]\n",
    "    ]\n",
    "\n",
    "    aggregated_correlations_df = pd.DataFrame(\n",
    "        aggregated_list, \n",
    "        columns=[\n",
    "            \"data_type\", \n",
    "            \"structure_category\", \n",
    "            \"tree_category\", \n",
    "            \"time_category\", \n",
    "            \"prediction_category\", \n",
    "            \"size\", \n",
    "            \"Kendall [c]\", \n",
    "            \"Kendall [p]\", \n",
    "            \"Spearman [c]\", \n",
    "            \"Spearman [p]\"\n",
    "        ]\n",
    "    )\n",
    "    if save:\n",
    "        aggregated_correlations_df.to_csv(f\"{ARTIFACTS_PATH}/aggregated_correlations_df.csv\", index=False)\n",
    "    return aggregated_correlations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(v, e, model):\n",
    "    v, e = preprocess(v, e)\n",
    "    return model(v.unsqueeze(0), e.unsqueeze(0)).squeeze(0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_query_correlations(runs_range, list_info, oracle):\n",
    "    correlations = []\n",
    "    for run in runs_range:\n",
    "        model, *_ = load_run(run)\n",
    "        costs, predictions, times = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "        \n",
    "        for info in list_info:\n",
    "            request = OracleRequest(query_name=info[\"query_name\"], hintset=info[\"hintset\"],dop=info[\"dop\"])\n",
    "            times[info[\"query_name\"]].append(info[\"time\"])\n",
    "            predictions[info[\"query_name\"]].append(get_prediction(info[\"vertices\"], info[\"edges\"], model))\n",
    "            costs[info[\"query_name\"]].append(oracle.get_cost(request))\n",
    "\n",
    "        kendall_c, kendall_p, spearman_c, spearman_p = [], [], [], []\n",
    "        for q_n in set(info[\"query_name\"] for info in list_info):\n",
    "            kendall_c.append(kendalltau(times[q_n], costs[q_n])[0])\n",
    "            kendall_p.append(kendalltau(times[q_n], predictions[q_n])[0])\n",
    "            spearman_c.append(spearmanr(times[q_n], costs[q_n])[0])\n",
    "            spearman_p.append(spearmanr(times[q_n], predictions[q_n])[0])    \n",
    "        \n",
    "        correlations.append((np.nanmean(kendall_c), np.nanmean(kendall_p), np.nanmean(spearman_c), np.nanmean(spearman_p)))\n",
    "    \n",
    "    return (\n",
    "        correlations_to_row([correlations[run-1][0] for run in runs_range]),\n",
    "        correlations_to_row([correlations[run-1][1] for run in runs_range]),\n",
    "        correlations_to_row([correlations[run-1][2] for run in runs_range]),\n",
    "        correlations_to_row([correlations[run-1][3] for run in runs_range]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimations_and_params(run, oracle):\n",
    "    q_n_to_predictions_and_params, q_n_to_costs_and_params = {}, {}\n",
    "    model, *_ = load_run(run)\n",
    "    for q_n in oracle.get_query_names():\n",
    "        top_k_by_costs, top_k_by_predictions = [], []\n",
    "        for hs in HINTSETS:\n",
    "            for dop in [DEFAULT_DOP]:\n",
    "                request = OracleRequest(query_name=q_n, hintset=hs, dop=dop)\n",
    "                v, e = extract_vertices_and_edges(oracle.get_explain_plan(request))\n",
    "                top_k_by_costs.append((oracle.get_cost(request), hs, dop))\n",
    "                top_k_by_predictions.append((get_prediction(v, e, model), hs, dop))\n",
    "        q_n_to_costs_and_params[q_n] = top_k_by_costs\n",
    "        q_n_to_predictions_and_params[q_n] = top_k_by_predictions\n",
    "    return q_n_to_predictions_and_params, q_n_to_costs_and_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e2e_performances(q_n_list, oracle, q_n_to_costs_and_params, q_n_to_predictions_and_params, k):\n",
    "    e2e_cost_time, e2e_pred_time, e2e_def_time = 0, 0, 0 \n",
    "    for q_n in q_n_list:\n",
    "        def_time = cost_time = pred_time = oracle.get_execution_time(OracleRequest(query_name=q_n, hintset=0, dop=DEFAULT_DOP))\n",
    "        cost_and_params = sorted(q_n_to_costs_and_params[q_n])\n",
    "        assert q_n in q_n_to_predictions_and_params, (q_n, q_n_to_predictions_and_params.keys())\n",
    "        pred_and_params = sorted(q_n_to_predictions_and_params[q_n])\n",
    "        \n",
    "        for pred, hs, dop in pred_and_params[:k]:\n",
    "            pred_time = min(pred_time, oracle.get_execution_time(OracleRequest(query_name=q_n, hintset=hs, dop=dop)))\n",
    "        \n",
    "        for cost, hs, dop in cost_and_params[:k]:\n",
    "            cost_time = min(cost_time, oracle.get_execution_time(OracleRequest(query_name=q_n, hintset=hs, dop=dop)))        \n",
    "            \n",
    "        e2e_cost_time += cost_time\n",
    "        e2e_pred_time += pred_time\n",
    "        e2e_def_time += def_time\n",
    "    return e2e_def_time / 1000, e2e_cost_time / 1000, e2e_pred_time / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_plot(\n",
    "    x, \n",
    "    def_times, \n",
    "    cost_times, \n",
    "    pred_times,\n",
    "    figsize=(8, 4), \n",
    "    save_name=None, \n",
    "    linewidth=1,\n",
    "    markersize=3,\n",
    "    label_fontsize=12, \n",
    "    tick_label_fontsize=10, \n",
    "    legend_fontsize=12, \n",
    "    cbar_label_fontsize=12,\n",
    "):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_palette(\"deep\")\n",
    "\n",
    "    assert len(set(def_times)) == 1\n",
    "    plt.plot([x[0], x[-1]], [def_times[0], def_times[0]], label='Default', linewidth=linewidth, markersize=markersize)\n",
    "    plt.plot(x, cost_times, label='Cost Model', marker='^', linewidth=linewidth, markersize=markersize)\n",
    "    plt.plot(x, pred_times, label='NN', marker='x', linewidth=linewidth, markersize=markersize)\n",
    "\n",
    "    plt.legend(fontsize=legend_fontsize)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=tick_label_fontsize)\n",
    "    plt.xlabel('k', fontsize=label_fontsize)\n",
    "    plt.ylabel('Execution Time (sec)', fontsize=label_fontsize)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_name:\n",
    "        plt.savefig(f\"{ARTIFACTS_PATH}/{save_name}\", format=\"svg\", bbox_inches=\"tight\", dpi=300)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and Statements about Generalisation\n",
    "\n",
    "Here we'll form statements around the question\n",
    "\n",
    "**\"What is the key to generalising knowledge through a BTCNN?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will study the NN errors in the simplest case - on the training data.\n",
    "\n",
    "**Observation:** all train data has been learned (at least memorized), but we have high MAPE on small queries due to overestimations, i.e. on train data relative error of NN's predictions may still be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(\n",
    "    df=aggregated_metrics_df, \n",
    "    allowed_data_type_predicates=[\"train\"], \n",
    "    allowed_structure_predicates=[\"seen\"], \n",
    "    allowed_tree_predicates=[\"seen\"],\n",
    "    allowed_time_predicates=[\"small\"],\n",
    "    allowed_prediction_predicates=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement #1: `T/O` lead to underestimation of the label, which complicates the landscape\n",
    "\n",
    "For queries with `T/O`, the label actually stops to reflect the actual dependency, and as a result the learned landscape could become much more complex. Indeed, if many long queries have similar plans but the baselines of their timeout values are very different, the NN will have to adjust for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_run_and_get_filtered_df(1, \"train\", \"all\", \"all\", \"all\", \"all\")\n",
    "train_df[\"abs_error\"] = train_df[\"error\"].abs()\n",
    "train_df[\"mape\"] = train_df[\"prediction\"] / train_df[\"time\"] .abs()\n",
    "train_df[\"is_dirty\"] = train_df.apply(lambda row: is_dirty(*preprocess(row[\"vertices\"], row[\"edges\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[\n",
    "    ((train_df[\"prediction_category\"] == \"overestimated\") & (train_df[\"is_dirty\"] == True)),\n",
    "    [\"mape\", \"time\", \"prediction\", \"frequency\", \"time_category\", \"is_dirty\", \"abs_error\"],\n",
    "].sort_values(by=[\"mape\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUTED_AND_OVERESTIMATED_IDX = 5177\n",
    "target_v, target_e, target_embedding, target_y = train_df.loc[TIMEOUTED_AND_OVERESTIMATED_IDX][[\"vertices\", \"edges\", \"embedding\", \"time\"]]\n",
    "support_points, all_distances = get_k_support_points_and_all_distances(target_embedding, train_dataloader, k=100)\n",
    "\n",
    "for d, v, e, f, t, p, emb in sorted(support_points, key=lambda x: x[0])[:10]:\n",
    "    print(f\"distance {d:0.3f} | real time {t:0.3f} | prediction {p:0.3f} | is_dirty={is_dirty(v, e)}\")\n",
    "plot_avg_knn_times(support_points)\n",
    "\n",
    "TIMEOUTED_AND_OVERESTIMATED_ARGS = prepare_args_for_3d_plot(\n",
    "    TIMEOUTED_AND_OVERESTIMATED_IDX, train_df, 20, 100, noise_level=.1\n",
    ")\n",
    "\n",
    "plot_3d_surface(\n",
    "    *TIMEOUTED_AND_OVERESTIMATED_ARGS, \n",
    "    elev=25, azim=30, figsize=(12, 6), softplus_rescale=False, \n",
    "    with_auxilary_points=True, projector=PCA, save_name=\"train_timeouted_overestimated.svg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Execution time of default plan is ~{job_oracle.get_execution_time(OracleRequest(query_name='30c', hintset=0, dop=1)) / 1000:0.3f} sec\")\n",
    "print(\"Queries and parameters that lead to target plan (query, hs, dop):\")\n",
    "[(q_n, hs, dop) for q_n, hs, dop, t in get_query_info(*preprocess(target_v, target_e), TRAIN_VE_TO_TIMES, TRAIN_VET_TO_QUERY_INFO, job_oracle)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the actual execution time for this query is ~14sec. This means that our processning `T/O` indeed led to **underestimation of the label** and this, in turn, created the situation above - next to the large labels we plan with a highly underestimated time. In fact, we can even say that overestimating such a label is a natural behaviour - because other similar queries clearly show that certain patterns from the plan are signals of slow performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement #2: in some regions the landscape is sharp by the nature of the task\n",
    "\n",
    "The very nature of the dependence under study may cause situations when fast queries are located with relatively long queries, which makes the task of approximating surface difficult. In the extreme case where plans are completely same, we called this phenomenon collision. Although we have already simplified the problem in WeightedDataset by handling all collisions, a weaker version of this phenomenon is still observed in the data - we have similar plans whose labels are significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[\n",
    "    ((train_df[\"prediction_category\"] == \"overestimated\") & (train_df[\"is_dirty\"] == False)),\n",
    "    [\"mape\", \"time\", \"prediction\", \"frequency\", \"time_category\", \"is_dirty\", \"abs_error\"],\n",
    "].sort_values(by=[\"mape\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOST_OVERESTIMATED_IDX = 3417\n",
    "target_v, target_e, target_embedding, target_y = train_df.loc[MOST_OVERESTIMATED_IDX][[\"vertices\", \"edges\", \"embedding\", \"time\"]]\n",
    "support_points, all_distances = get_k_support_points_and_all_distances(target_embedding, train_dataloader, k=100)\n",
    "\n",
    "for d, v, e, f, t, p, emb in sorted(support_points, key=lambda x: x[0])[:10]:\n",
    "    print(f\"distance {d:0.3f} | real time {t:0.3f} | prediction {p:0.3f} | is_dirty={is_dirty(v, e)}\")\n",
    "plot_avg_knn_times(support_points)\n",
    "\n",
    "MOST_OVERESTIMATED_ARGS = prepare_args_for_3d_plot(\n",
    "    MOST_OVERESTIMATED_IDX, train_df, 15, 20, noise_level=1\n",
    ")\n",
    "\n",
    "plot_3d_surface(\n",
    "    *MOST_OVERESTIMATED_ARGS, \n",
    "    elev=0, azim=75, figsize=(12, 6), softplus_rescale=False, \n",
    "    with_auxilary_points=True, projector=PCA, save_name=\"train_overestimated.svg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.**\\\n",
    "Everything appears as though the smoothness of the surface approximated by NN leads to a situation where points on the slopes of a sharp peak are underestimated in order to “catch up” and approach a sufficiently small value at its base. And due to the struggle of three forces - a) needing a sufficiently large value at the peak, b) not too small on the slope, b) and very small at the elevation, we end up with underestimation and overestimation in the latter two cases, respectively.\n",
    "\n",
    "P.S. it is also difficult to quickly ‘descend’ from such hills because the dynamics of weight changes in the ‘lowlands’ is strongly slowed down due to the gradient (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement #3: the dynamics of learning at points with small predictions is difficult by itself\n",
    "\n",
    "To make the predictions positive and at the same time be able to get feedback from small predictions during back propagation, we decided to add activation at the end of the NN and chose the softplus layer instead of relu. But this has consequences - the derivative of the softplus layer for small predictions is very small, so the learning speed in these areas may lead to overestimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[\n",
    "    ((train_df[\"prediction_category\"] == \"overestimated\") & (train_df[\"is_dirty\"] == False)),\n",
    "    [\"mape\", \"time\", \"prediction\", \"frequency\", \"time_category\", \"prediction_category\", \"embedding\", \"is_dirty\", \"abs_error\"]\n",
    "].sort_values(by=[\"mape\"], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENT_AND_OVERESTIMATED_IDX = 4778\n",
    "target_v, target_e, target_embedding, target_y = train_df.loc[FREQUENT_AND_OVERESTIMATED_IDX][[\"vertices\", \"edges\", \"embedding\", \"time\"]]\n",
    "support_points, all_distances = get_k_support_points_and_all_distances(target_embedding, train_dataloader, k=100)\n",
    "\n",
    "for d, v, e, f, t, p, emb in sorted(support_points, key=lambda x: x[0])[:10]:\n",
    "    print(f\"distance {d:0.3f} | real time {t:0.3f} | prediction {p:0.3f} | is_dirty={is_dirty(v, e)}\")\n",
    "plot_avg_knn_times(support_points)\n",
    "\n",
    "FREQUENT_AND_OVERESTIMATED_ARGS = prepare_args_for_3d_plot(\n",
    "    FREQUENT_AND_OVERESTIMATED_IDX, train_df, 20, 100, noise_level=.1\n",
    ")\n",
    "\n",
    "plot_3d_surface(\n",
    "    *FREQUENT_AND_OVERESTIMATED_ARGS, \n",
    "    elev=0, azim=75, figsize=(12, 6), softplus_rescale=True, \n",
    "    with_auxilary_points=True, projector=PCA, save_name=\"train_frequent_overestimated.svg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - we visualized the dependency before including the final `softplus` layer \n",
    "\n",
    "$$y := \\text{NN}(x) = \\text{softplus}(\\text{FCNN}(\\text{BTCNN}(x)))$$\n",
    "$$\\hat{y} := \\text{FCNN}(\\text{BTCNN}(x)), \\text{ i.e. } y = \\text{softplus}(\\hat{y})$$\n",
    "\n",
    "This visualization revealed difficulties in predicting small values across most scenarios, likely due to this last layer. Specifically, the derivative of the `softplus` is $\\frac{1}{1+e^{-\\hat{y}}}$, and when predicting $y \\approx 0.2$, $\\hat{y}$ values close to 2.5. So, in that regions the gradient is reduced in **>10 times**. Consequently, with a small `lr` in the later stages of training, it becomes **challenging to adjust weights effectively** in such demanding areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "$metric\\vert_{train} \\ll metric\\vert_{val}$ for **ANY** metric on **ANY** subset\n",
    "\n",
    "Is seems that there's no possiblity for complete generalization. Cause of that complex dependency between plans and times it may be impossible to correctly interpolate knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"train\"], [\"all\"], [\"all\"], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"val\"], [\"all\"], [\"all\"], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement #4: the sharpness of the landscape is one of the challenges in generalisation\n",
    "\n",
    "Probably most often errors are reached in the \"peaks\" before mountains with the transition of fast queries to long queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = load_run_and_get_filtered_df(1, \"val\", \"all\", \"all\", \"all\", \"all\")\n",
    "val_df[\"abs_error\"] = val_df[\"error\"].abs()\n",
    "val_df[\"mape\"] = val_df[\"prediction\"] / val_df[\"time\"] .abs()\n",
    "val_df[\"is_dirty\"] = val_df.apply(lambda row: is_dirty(*preprocess(row[\"vertices\"], row[\"edges\"])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**most overestimated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.loc[\n",
    "    (val_df[\"is_dirty\"] == False) & (val_df[\"prediction_category\"] == \"overestimated\"),\n",
    "    [\"abs_error\", \"time\", \"prediction\", \"frequency\", \"time_category\", \"prediction_category\", \"embedding\", \"is_dirty\", \"mape\"]\n",
    "].sort_values(by=\"abs_error\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOST_OVERESTIMATED_IDX = 6182\n",
    "target_v, target_e, target_embedding, target_y = val_df.loc[MOST_OVERESTIMATED_IDX][[\"vertices\", \"edges\", \"embedding\", \"time\"]]\n",
    "support_points, all_distances = get_k_support_points_and_all_distances(target_embedding, train_dataloader, k=100)\n",
    "\n",
    "for d, v, e, f, t, p, emb in sorted(support_points, key=lambda x: x[0])[:10]:\n",
    "    print(f\"distance {d:0.3f} | real time {t:0.3f} | prediction {p:0.3f} | is_dirty={is_dirty(v, e)}\")\n",
    "plot_avg_knn_times(support_points)\n",
    "\n",
    "MOST_OVERESTIMATED_ARGS = prepare_args_for_3d_plot(\n",
    "    MOST_OVERESTIMATED_IDX, val_df, 20, 100, noise_level=.1\n",
    ")\n",
    "\n",
    "plot_3d_surface(\n",
    "    *MOST_OVERESTIMATED_ARGS, \n",
    "    elev=25, azim=65, figsize=(12, 6), softplus_rescale=False, \n",
    "    with_auxilary_points=True, projector=PCA, save_name=\"val_overestimated.svg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.**\\\n",
    "We see, that many close plans are slow and `T/O`ed, and learned landscape isn't sharp enough to predict so small value (3sec) on the slope. In fact, the neural network had no information that this peak was so sharp, **so its behaviour is completely justified by the train data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**most underestimated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.loc[\n",
    "    (val_df[\"is_dirty\"] == False) & (val_df[\"prediction_category\"] == \"underestimated\"),\n",
    "    [\"abs_error\", \"time\", \"prediction\", \"frequency\", \"time_category\", \"prediction_category\", \"embedding\", \"is_dirty\", \"mape\"]\n",
    "].sort_values(by=\"abs_error\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOST_UNDERESTIMATED_IDX = 6291\n",
    "target_v, target_e, target_embedding, target_y = val_df.loc[MOST_UNDERESTIMATED_IDX][[\"vertices\", \"edges\", \"embedding\", \"time\"]]\n",
    "support_points, all_distances = get_k_support_points_and_all_distances(target_embedding, train_dataloader, k=100)\n",
    "\n",
    "for d, v, e, f, t, p, emb in sorted(support_points, key=lambda x: x[0])[:10]:\n",
    "    print(f\"distance {d:0.3f} | real time {t:0.3f} | prediction {p:0.3f} | is_dirty={is_dirty(v, e)}\")\n",
    "plot_avg_knn_times(support_points)\n",
    "\n",
    "MOST_UNDERESTIMATED_ARGS = prepare_args_for_3d_plot(\n",
    "    MOST_UNDERESTIMATED_IDX, val_df, 20, 100, noise_level=.1\n",
    ")\n",
    "\n",
    "plot_3d_surface(\n",
    "    *MOST_UNDERESTIMATED_ARGS, \n",
    "    elev=5, azim=45, figsize=(12, 6), softplus_rescale=False, \n",
    "    with_auxilary_points=True, projector=PCA, save_name=\"val_underestimated.svg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.**\\\n",
    "We see, there're just no points with such big execution time in train data, so **NN virtually hadn't opportunity to predict that**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement #5: tree structure helps in generalization\n",
    "We can see that $metric\\vert_{seen\\_tree} \\ll metric\\vert_{unseen\\_tree}$ for **ANY** metric on **ANY** subset, i.e. tree stucture helps NN in generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"val\"], [\"seen\"], [\"all\"], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"val\"], [\"unseen\"], [\"all\"], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement #6 (from `learning_dynamic.ipynb`): statistics are more descriptive feature than tree structure\n",
    "\n",
    "This is not surprising, as statistics already include data size information, which is crucial for time prediction. Essentially, the neural network should act as a global cost model in this scenario. When information about the tree structure is available, it can transform into a contextual cost model, adjusting global cost parameters based on the specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(interesting!) Observation:**\n",
    "\n",
    "$metric\\vert_{test} \\ll metric\\vert_{val}$ for almost **ANY** metric on **ANY** subset\n",
    "\n",
    "Probably, on average, the data in the test is similar to a huge number of queries from the train, and with any split, most of it is well represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"test\"], [\"all\"], [\"all\"], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"val\"], [\"all\"], [\"all\"], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It led us to the next discovery:\n",
    "\n",
    "### Statement #7: **sub**structures are also important for generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_neighborhoods_df[aggregated_neighborhoods_df[\"data_type\"] == \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_neighborhoods_df[aggregated_neighborhoods_df[\"data_type\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.**\\\n",
    "We see that, on average, the probability of encountering subtrees of height 1-4 in the test is significantly higher. This must have a positive effect on the borrowing of knowledge from the train and allows for better prediction on the test.\n",
    "\n",
    "Note that there is no point in considering trees of greater height - as we only use 4 tree convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(interesting!) Observation:**\n",
    "\n",
    "$mse\\vert_{underestimated} \\ll mse\\vert_{overestimated}$\n",
    "\n",
    "i.e. the main problem with `ood` data is huge underestimation of super-long queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"ood\"], [\"all\"], [\"all\"], [\"all\"], [\"all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"ood\"], [\"all\"], [\"all\"], [\"big\"], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"ood\"], [\"all\"], [\"all\"], [\"medium\"], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_df(aggregated_metrics_df, [\"ood\"], [\"all\"], [\"all\"], [\"small\"], None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement #8: if there is a shift in the time distribution, the model loses the ability to generalise\n",
    "\n",
    "We have already seen how underestimation of multiple long queries significantly increases MSE on the val data. In the case of ood, this situation becomes even more acute - we have a larger number of long queries in general, which in addition have a unseen tree structure. All this leads to the fact that training on ood data is almost non-existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times = [el[3].to(\"cpu\").item() for el in train_dataset for _ in range(el[2])]\n",
    "val_times = [el[3].to(\"cpu\").item() for el in val_dataset for _ in range(el[2])]\n",
    "\n",
    "test_times = [el[3].to(\"cpu\").item() for el in test_dataset for _ in range(el[2])]\n",
    "ood_times = [el[3].to(\"cpu\").item() for el in ood_dataset for _ in range(el[2])]\n",
    "\n",
    "plot_times(train_times, test_times, val_times, ood_times, save_name=\"time_distribution.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement 9: correlation between prediction and execution time for NN is better than for Cost Model on `val` and `test` samples, but loses on `ood`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments with noised data have shown us that NN probably plays the role of a cost-corrector (where the unit of cost is time). Hence, we wonder - in the most ‘inconvenient’ case on `ood` data, even if the exact time cannot be predicted, which predictions are better to trust - from NN or from Planner?\n",
    "\n",
    "P.S. hint mechanism uses **cost corruption** (at least in `@OpenGauss` and `@PostgreSQL`), so the cost model should be better at the normal planning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 5\n",
    "correlations_dict = calculate_stratified_correlations(n_runs)\n",
    "correlations_df = aggregate_correlations_dict(correlations_dict,  range(1, n_runs+1))\n",
    "# pd.read_csv(f\"{ARTIFACTS_PATH}/correlations_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_subdf(correlations_df, [\"train\"], [\"all\"], [\"all\"], None, [\"all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_subdf(correlations_df, [\"val\"], [\"all\"], [\"all\"], None, [\"all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_subdf(correlations_df, [\"test\"], [\"all\"], [\"all\"], None, [\"all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_subdf(correlations_df, [\"ood\"], [\"all\"], [\"all\"], None, [\"all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_params = [\n",
    "    (\"trainval\", job_list_info, job_oracle),\n",
    "    (\"test\", id_sq_list_info, sq_oracle),\n",
    "    (\"ood\", ood_sq_list_info, sq_oracle)\n",
    "]\n",
    "\n",
    "data = [(d, *get_per_query_correlations(range(1, 5+1), l, o)) for d, l, o in metric_params]\n",
    "columns = [\"data_type\",  \"Kendall [c]\",  \"Kendall [p]\",  \"Spearman [c]\",  \"Spearman [p]\"]\n",
    "per_query_correlations_df = pd.DataFrame(data, columns=columns)\n",
    "per_query_correlations_df.to_csv(f\"{ARTIFACTS_PATH}/per_query_correlations_df.csv\", index=False)\n",
    "# per_query_correlations_df = pd.read_csv(f\"{ARTIFACTS_PATH}/per_query_correlations_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_correlations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.**\\\n",
    " The per-query analysis confirmed our fears that generalisation on `ood` data might be quite weak. We saw that mse hardly dropped at all on `ood` data, and the current results also show us that in comparing an arbitrary pair of plans for specific queries, on average, the cost model will perform better on `ood` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement 10: NN can be used for solving Ranking Problem (except `ood` data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, ranking problem is interesting for its own reason, because the ranking problem by its nature can be a bit easier than a regression problem - we do not care how much the model is wrong, the only thing that matters is that the correct order is restored. An example of such a problem is the ranking of neighbours in an iteration of a local search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the total execution time of queries after optimisation, which consists in the following: execute the top-k queries and store the best one, where the plans order will be induced either a) by cost predictions or b) by NN predictions.\n",
    "\n",
    "Baseline is default planner with `dop` equal to 64. All calculations are made on data and model from the first split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_q_n_to_predictions_and_params, job_q_n_to_costs_and_params = get_estimations_and_params(1, job_oracle)\n",
    "sq_q_n_to_predictions_and_params, sq_q_n_to_costs_and_params = get_estimations_and_params(1, sq_oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_queries = job_oracle.get_query_names()\n",
    "test_queries, ood_queries = set(info[\"query_name\"] for info in id_sq_list_info), set(info[\"query_name\"] for info in ood_sq_list_info)\n",
    "assert not test_queries & ood_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on `train`/`val` data is impossible because data has been splitted for training in per-plan way.\n",
    "k_range = range(0, len(HINTSETS))\n",
    "trainval_def, trainval_cost, trainval_pred = zip(*[get_e2e_performances(trainval_queries, job_oracle, job_q_n_to_costs_and_params, job_q_n_to_predictions_and_params, k) for k in k_range])\n",
    "test_def, test_cost, test_pred = zip(*[get_e2e_performances(test_queries, sq_oracle, sq_q_n_to_costs_and_params, sq_q_n_to_predictions_and_params, k) for k in k_range])\n",
    "ood_def, ood_cost, ood_pred = zip(*[get_e2e_performances(ood_queries, sq_oracle, sq_q_n_to_costs_and_params, sq_q_n_to_predictions_and_params, k) for k in k_range])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_plot(k_range, trainval_def, trainval_cost, trainval_pred, save_name=\"topk_performance_trainval.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_plot(k_range, test_def, test_cost, test_pred, save_name=\"topk_performance_test.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_plot(k_range, ood_def, ood_cost, ood_pred, save_name=\"topk_performance_ood.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions.** \\\n",
    "On `ood` data the NN performs slightly worse than the standard Cost Model, but on `train`, `val` and `test` data the predictions of the model are **significantly more efficient**. This allows us to hope that when solving the regression problem, the NN predictions can be successfully used to optimise queries at least for solving ranking problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
